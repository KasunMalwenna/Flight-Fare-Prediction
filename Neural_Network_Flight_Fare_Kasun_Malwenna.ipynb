{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x41rNVzDtlDQ"
   },
   "source": [
    "<font size=\"7\"><center>Flight Fare Prediction</center></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><center> **Neural Network Model**</center></font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "<font size=\"2\">__Author__: -  Kasun Malwenna</font>\n",
    "\n",
    "<font size=\"2\">__Contact__: - kasunmalwenna@gmail.com</font>\n",
    "    \n",
    "---\n",
    "    \n",
    "This notebook only consists of modelling and evaluating Deep Learning model using the cleaned version of the data. \n",
    "\n",
    "For cleaning and preprocessing steps, refer to **\"_Data_Wrangling_and_EDA__Kasun_Malwenna.ipynb_\"** notebook. For Machine Learning models, refer to **\"_Modeling_Flight_Fare_Kasun_Malwenna.ipynb_\"** notebook.\n",
    "\n",
    "This notebook’s outputs cannot be reproduced unless run in a specific Colab environment.\n",
    "    \n",
    "---  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "># Working In Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network is essentially several basic ML models strung together that have excellent predictive capabilities and can handle both structured and unstructured data. \n",
    "An NN architecture comprises an inter-connected Input layer, hidden layers and an output layer. \n",
    "\n",
    "Parameters of NN refer to the weights of the connection between these layers, and weights are learned and tuned during the learning stage of the model. The computational complexity of a NN can greatly increase with the increase in the number of parameters. Therefore, NN can greatly benefit from using faster GPUs during training.\n",
    "\n",
    "Google Colab is a could service that offers a variety of computer resources such as faster CPUs, GPUs and RAMs and a jupyter notebook style environment for machine learning tasks. We will use Colab and their premium GPUs to train our NN model efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to enable GPUs in Colab by changing the runtime to “Premium GPU”. Then we are required to use TensorFlow 2.10.0 which is the only version that supports GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OysIT66CAJAc",
    "outputId": "500a2d06-e278-4f0c-af69-b1e47c6a5f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 02:02:45 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    43W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if we are connected to a GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gx8q_VK_tquX",
    "outputId": "3beae8fd-1582-4469-bc49-d41a00cdc23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 578.0 MB 16 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 75.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.27.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.50.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 92.0 MB/s \n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 85.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (57.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.19.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.38.3)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2022.9.24)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow-gpu\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 22.10.26 which is incompatible.\n",
      "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow 2.9.2 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.10.0 which is incompatible.\u001b[0m\n",
      "Successfully installed flatbuffers-22.10.26 keras-2.10.0 tensorboard-2.10.1 tensorflow-estimator-2.10.0 tensorflow-gpu-2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Instale tensofor gpu version in colab environment\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQSnITHJtroK",
    "outputId": "6687dcc1-b185-41cd-f936-43d7ebc493eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow and check version.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s import the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aHJmu0vlt1Rf"
   },
   "outputs": [],
   "source": [
    "# Importing standard data science libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing required tensorflow moules\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Importing required library to save and load models\n",
    "import joblib\n",
    "\n",
    "# Import required colab modules to import and load data to a dataframe\n",
    "from google.colab import files\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "># Pre-modeling Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working in a cloud setting, we have to use Colabs built-in functions to upload and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "2G_o1kB_t4SU",
    "outputId": "f9ca0036-76c3-495b-85de-2770c5068db0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3c491abd-0e5e-4786-9728-b3d749e65846\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-3c491abd-0e5e-4786-9728-b3d749e65846\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test_data.csv to test_data.csv\n",
      "Saving train_data.csv to train_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Uploading the train and test data\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T0savHMWt6km"
   },
   "outputs": [],
   "source": [
    "# Loading the datasets into dataframes\n",
    "train_df = pd.read_csv(io.BytesIO(uploaded['train_data.csv']))\n",
    "test_df = pd.read_csv(io.BytesIO(uploaded['test_data.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZrF1bRLdt9aY"
   },
   "outputs": [],
   "source": [
    "# Defining the dependent and independent variables for train and test sets\n",
    "\n",
    "X_train= train_df.drop(columns= ['price']) # independent\n",
    "y_train = train_df['price']                # dependent\n",
    "\n",
    "X_test= test_df.drop(columns= ['price'])   # independent\n",
    "y_test = test_df['price']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UD5kyAJ3t_8a",
    "outputId": "bf063898-aba6-41ae-d6ca-551d708ae06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (240207, 22)\n",
      "y_train shape (240207,)\n",
      "X_test shape (60052, 22)\n",
      "y_test shape (60052,)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes\n",
    "print(f'X_train shape {X_train.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'X_test shape {X_test.shape}')\n",
    "print(f'y_test shape {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since NN is a decent-based algorithm, it is always best practice to scale the data first to remove any arbitrary effects from differences in feature scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wzL-GgW7uCOn"
   },
   "outputs": [],
   "source": [
    "# Scale data using StandardScaler\n",
    "\n",
    "# Instantiate and fit the scaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_ss = ss.transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the same metrics and evaluation function for this model. However, we will rename the evaluation function to avoid any conflicts that may arise since TensorFlow already has a built-in “evaluate” method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z9hkwXquuOqJ"
   },
   "outputs": [],
   "source": [
    "# Difining a function to evaluate the models\n",
    "\n",
    "def evaluate_NN(model, X_test, y_test, X_train, y_train): # Renamed the previously used function\n",
    "    \n",
    "    # Empty list to record metrics\n",
    "    performance_metrics = []\n",
    "    \n",
    "    # ake predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Append model name\n",
    "    performance_metrics.append(model)\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('-----------------')\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    \n",
    "    # coefficient of determination\n",
    "    train_r2 = round(metrics.r2_score(y_train,y_pred_train),4)\n",
    "    print(f\"Train set R2 score: {train_r2}\")\n",
    "    test_r2 = round(metrics.r2_score(y_test,y_pred_test),4)\n",
    "    performance_metrics.append(test_r2)\n",
    "    print(f\"Test R2 score: {test_r2}\")\n",
    "    \n",
    "    # Adjusted r2\n",
    "    # Define variables to calculate adjusted r2 score\n",
    "    r2 = metrics.r2_score(y_test,y_pred_test) #The R2 of the model\n",
    "    n = len(y_test)                      #The number of observations\n",
    "    k = X_test.shape[1]                  #The number of predictor variables\n",
    "    \n",
    "    # calculate adjusted r2 score\n",
    "    adj_r2_score = round(1-(((1-r2)*(n-1))/(n-k-1)),4)\n",
    "    performance_metrics.append(adj_r2_score)\n",
    "    print(f\"Test set Adjusted R2 score: {adj_r2_score}\")\n",
    "    \n",
    "    # Root Mean Squared Error\n",
    "    RMSE = round(metrics.mean_squared_error(y_test,y_pred_test, squared=False),4)\n",
    "    performance_metrics.append(RMSE)\n",
    "    print(f\"Test set Root Mean Squared Error - RMSE: {RMSE}\")\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    MAE = round(metrics.mean_absolute_error(y_test,y_pred_test),4)\n",
    "    performance_metrics.append(MAE)\n",
    "    print(f\"Test set Mean Absolute Error - MAE: {MAE}\")\n",
    "    \n",
    "    # Mean Absolute Precentage Error\n",
    "    MAPE = round(metrics.mean_absolute_percentage_error(y_test, y_pred_test),4)\n",
    "    performance_metrics.append(MAPE)\n",
    "    print(f\"Test set Mean Absolute Precentage Error - MAPE: {MAPE}\")\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "># Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s build a basic NN with an input layer of 32 nodes and one hidden layer of 512 nodes. We will use the “Relu” activation function as it is the most commonly used activation. For the output, we do not need to specify an activation, as the default output layer is linearly activated and returns a continuous value. However, we are sure our output variable (\"price\") can not be a negative value. Therefore we can you “Relu” for output activation as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four steps in building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1__ Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJVqF1iDuZ0Z"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(123) # for consistent results\n",
    "\n",
    "# Create a new sequential model\n",
    "NN_baseline_model = keras.Sequential()\n",
    "\n",
    "# Declare the input layer\n",
    "NN_baseline_model.add(layers.Dense(32, activation='relu', input_shape= (22,),  name='layer1'))\n",
    "\n",
    "# Declare the hidden layer\n",
    "NN_baseline_model.add(layers.Dense(512, activation='relu', name='hidden1'))\n",
    "NN_baseline_model.add(layers.Dropout(0.2)),\n",
    "\n",
    "\n",
    "# Declare the output layer\n",
    "NN_baseline_model.add(layers.Dense(1, activation='relu', name='output')) # ReLU since price cannot have negative values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the structure of our model and how many trainable parameters we can optimize the model on. Higher the number of trainable params, the better predictive accuracy we will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5emcCgfR13pP",
    "outputId": "2092161e-18d4-41b3-88e3-746d3b485f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 32)                736       \n",
      "                                                                 \n",
      " hidden1 (Dense)             (None, 512)               16896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,145\n",
      "Trainable params: 18,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Looking at model summary\n",
    "NN_baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__ Compile the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model compile is the step where we define the optimizer and the loss function to optimize the model on.\n",
    "\n",
    "optimizers are Classes or methods used to change the attributes of our (weights and learning rates) model to minimize the loss. Choosing the optimizer can mean a good result in seconds, hours or days.  Adam optimizer calls for fewer trainable parameters and has faster compute times with good accuracy. Therefore, it is recommended as a default for most of the problems.\n",
    "\n",
    "Since we are dealing with larger target values, the mean squared error (MSE) can be large. Therefore, we will use mean absolute error (MAE) as the loss function and monitor the MAPE and MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "514L3WuDveUt"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "NN_baseline_model.compile( \n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizer\n",
    "  loss=keras.losses.MeanAbsoluteError(), # Loss function to minimize\n",
    "  metrics=['mape', 'mse'] # metrics\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH8Y-0RDvg9G"
   },
   "source": [
    "__Step 3__ Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training ML models, we will only train the NN on train data.\n",
    "\n",
    "There are a number of parameters we can specify during the fitting of the model;\n",
    "- epochs - Number of repetitions of adjusting weights, calculation activations and making predictions\n",
    "- validation_split - separate a portion of the training data to evaluate the model after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pa1qMO1KvyYP",
    "outputId": "df8dde52-1ef3-48fc-a52b-6cce3197803d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6006/6006 [==============================] - 22s 3ms/step - loss: 8231.1699 - mape: 58.4944 - mse: 193757440.0000 - val_loss: 4586.5234 - val_mape: 30.4509 - val_mse: 52995784.0000\n",
      "Epoch 2/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 4242.5098 - mape: 29.5121 - mse: 45142108.0000 - val_loss: 3810.9514 - val_mape: 27.0230 - val_mse: 37043516.0000\n",
      "Epoch 3/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3794.8801 - mape: 28.7423 - mse: 35654164.0000 - val_loss: 3515.4058 - val_mape: 25.9989 - val_mse: 31988406.0000\n",
      "Epoch 4/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3676.2976 - mape: 28.9890 - mse: 33140130.0000 - val_loss: 3409.6172 - val_mape: 25.4941 - val_mse: 30600478.0000\n",
      "Epoch 5/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3609.2087 - mape: 28.8767 - mse: 31960438.0000 - val_loss: 3335.2556 - val_mape: 24.8591 - val_mse: 29734974.0000\n",
      "Epoch 6/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3553.1143 - mape: 28.6349 - mse: 31114778.0000 - val_loss: 3284.8833 - val_mape: 24.9745 - val_mse: 28668362.0000\n",
      "Epoch 7/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3504.8047 - mape: 28.3515 - mse: 30498120.0000 - val_loss: 3229.5332 - val_mape: 23.9639 - val_mse: 28272082.0000\n",
      "Epoch 8/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3465.7693 - mape: 27.9727 - mse: 30031026.0000 - val_loss: 3200.9429 - val_mape: 24.4919 - val_mse: 27653702.0000\n",
      "Epoch 9/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3438.6030 - mape: 27.7847 - mse: 29695802.0000 - val_loss: 3210.6670 - val_mape: 25.8949 - val_mse: 27519268.0000\n",
      "Epoch 10/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3409.1589 - mape: 27.5531 - mse: 29406828.0000 - val_loss: 3134.3147 - val_mape: 23.4711 - val_mse: 27213776.0000\n",
      "Epoch 11/100\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 3373.9390 - mape: 27.2514 - mse: 29029470.0000 - val_loss: 3119.8738 - val_mape: 23.7424 - val_mse: 26964966.0000\n",
      "Epoch 12/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3354.6750 - mape: 27.1630 - mse: 28877164.0000 - val_loss: 3087.8591 - val_mape: 24.0330 - val_mse: 26506244.0000\n",
      "Epoch 13/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3312.4607 - mape: 26.9446 - mse: 28339350.0000 - val_loss: 3043.8018 - val_mape: 23.4258 - val_mse: 26228312.0000\n",
      "Epoch 14/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3277.3665 - mape: 26.7575 - mse: 28009484.0000 - val_loss: 3017.7268 - val_mape: 23.5142 - val_mse: 25922368.0000\n",
      "Epoch 15/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3242.4678 - mape: 26.4851 - mse: 27536244.0000 - val_loss: 2980.4312 - val_mape: 22.6645 - val_mse: 25504318.0000\n",
      "Epoch 16/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3212.9092 - mape: 26.2576 - mse: 27207652.0000 - val_loss: 2948.1416 - val_mape: 23.2380 - val_mse: 25021244.0000\n",
      "Epoch 17/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3185.6001 - mape: 26.1370 - mse: 26837652.0000 - val_loss: 2927.0815 - val_mape: 22.8961 - val_mse: 24715024.0000\n",
      "Epoch 18/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3159.5500 - mape: 25.8582 - mse: 26525270.0000 - val_loss: 2905.8704 - val_mape: 22.7945 - val_mse: 24438318.0000\n",
      "Epoch 19/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3133.2617 - mape: 25.6642 - mse: 26230906.0000 - val_loss: 2892.4238 - val_mape: 22.7180 - val_mse: 24268664.0000\n",
      "Epoch 20/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3119.8391 - mape: 25.4920 - mse: 26204736.0000 - val_loss: 2882.0718 - val_mape: 22.7130 - val_mse: 24249450.0000\n",
      "Epoch 21/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3107.2253 - mape: 25.4430 - mse: 26094092.0000 - val_loss: 2866.5007 - val_mape: 22.4796 - val_mse: 24048308.0000\n",
      "Epoch 22/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3081.5537 - mape: 25.1300 - mse: 25896758.0000 - val_loss: 2826.6917 - val_mape: 21.9717 - val_mse: 23876988.0000\n",
      "Epoch 23/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 3042.5198 - mape: 24.3878 - mse: 25579186.0000 - val_loss: 2795.4543 - val_mape: 21.4227 - val_mse: 23481074.0000\n",
      "Epoch 24/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2983.9102 - mape: 23.3477 - mse: 25051810.0000 - val_loss: 2725.1006 - val_mape: 20.7211 - val_mse: 22757582.0000\n",
      "Epoch 25/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2927.3594 - mape: 22.5647 - mse: 24646150.0000 - val_loss: 2690.0776 - val_mape: 19.8219 - val_mse: 22542820.0000\n",
      "Epoch 26/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2903.7283 - mape: 22.1466 - mse: 24432094.0000 - val_loss: 2678.1226 - val_mape: 19.0333 - val_mse: 22823508.0000\n",
      "Epoch 27/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2884.8423 - mape: 21.8518 - mse: 24308710.0000 - val_loss: 2652.6575 - val_mape: 19.4845 - val_mse: 22436484.0000\n",
      "Epoch 28/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2862.5146 - mape: 21.5969 - mse: 24125752.0000 - val_loss: 2641.1704 - val_mape: 19.2282 - val_mse: 22325248.0000\n",
      "Epoch 29/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2851.0383 - mape: 21.4144 - mse: 24057726.0000 - val_loss: 2624.5420 - val_mape: 18.5922 - val_mse: 22433950.0000\n",
      "Epoch 30/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2841.1064 - mape: 21.2597 - mse: 24029000.0000 - val_loss: 2615.6897 - val_mape: 18.8148 - val_mse: 22086888.0000\n",
      "Epoch 31/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2831.2107 - mape: 21.1257 - mse: 23993788.0000 - val_loss: 2613.0596 - val_mape: 18.3945 - val_mse: 22257600.0000\n",
      "Epoch 32/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2822.8027 - mape: 20.9838 - mse: 23853694.0000 - val_loss: 2608.1582 - val_mape: 18.3342 - val_mse: 22328132.0000\n",
      "Epoch 33/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2815.7275 - mape: 20.8636 - mse: 23924880.0000 - val_loss: 2603.7161 - val_mape: 18.3367 - val_mse: 22349518.0000\n",
      "Epoch 34/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2806.7898 - mape: 20.7311 - mse: 23870344.0000 - val_loss: 2596.9939 - val_mape: 18.2061 - val_mse: 22461186.0000\n",
      "Epoch 35/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2795.5295 - mape: 20.6447 - mse: 23746774.0000 - val_loss: 2598.2131 - val_mape: 18.3170 - val_mse: 22361780.0000\n",
      "Epoch 36/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2790.5183 - mape: 20.5142 - mse: 23748940.0000 - val_loss: 2597.8525 - val_mape: 18.3437 - val_mse: 22076970.0000\n",
      "Epoch 37/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2780.9226 - mape: 20.3921 - mse: 23717832.0000 - val_loss: 2580.8677 - val_mape: 18.3415 - val_mse: 21985516.0000\n",
      "Epoch 38/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2776.9692 - mape: 20.3115 - mse: 23735572.0000 - val_loss: 2581.5479 - val_mape: 18.3990 - val_mse: 22101054.0000\n",
      "Epoch 39/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2768.0051 - mape: 20.1937 - mse: 23701716.0000 - val_loss: 2577.4065 - val_mape: 18.1591 - val_mse: 22096712.0000\n",
      "Epoch 40/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2763.2378 - mape: 20.1156 - mse: 23709378.0000 - val_loss: 2576.6125 - val_mape: 17.9251 - val_mse: 22253382.0000\n",
      "Epoch 41/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2754.8198 - mape: 20.0425 - mse: 23588158.0000 - val_loss: 2598.9275 - val_mape: 19.2186 - val_mse: 21961008.0000\n",
      "Epoch 42/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2746.6042 - mape: 19.9404 - mse: 23576176.0000 - val_loss: 2557.7720 - val_mape: 18.2123 - val_mse: 21864916.0000\n",
      "Epoch 43/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2737.1147 - mape: 19.8258 - mse: 23516788.0000 - val_loss: 2551.8855 - val_mape: 18.0308 - val_mse: 21899586.0000\n",
      "Epoch 44/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2732.4114 - mape: 19.7199 - mse: 23532066.0000 - val_loss: 2556.8726 - val_mape: 17.9331 - val_mse: 22340540.0000\n",
      "Epoch 45/100\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2724.6001 - mape: 19.6303 - mse: 23519140.0000 - val_loss: 2551.0989 - val_mape: 18.3244 - val_mse: 21872334.0000\n",
      "Epoch 46/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2718.6897 - mape: 19.5746 - mse: 23408070.0000 - val_loss: 2535.0720 - val_mape: 17.7770 - val_mse: 21952554.0000\n",
      "Epoch 47/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2712.5093 - mape: 19.4977 - mse: 23433670.0000 - val_loss: 2549.5515 - val_mape: 17.7293 - val_mse: 22240602.0000\n",
      "Epoch 48/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2710.0044 - mape: 19.3943 - mse: 23470314.0000 - val_loss: 2541.2991 - val_mape: 17.9733 - val_mse: 21943252.0000\n",
      "Epoch 49/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2702.8489 - mape: 19.2882 - mse: 23388290.0000 - val_loss: 2534.0632 - val_mape: 17.6898 - val_mse: 21997056.0000\n",
      "Epoch 50/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2696.7729 - mape: 19.2277 - mse: 23324398.0000 - val_loss: 2537.5391 - val_mape: 18.0272 - val_mse: 22012806.0000\n",
      "Epoch 51/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2690.3633 - mape: 19.1260 - mse: 23299428.0000 - val_loss: 2545.3479 - val_mape: 18.2058 - val_mse: 21978536.0000\n",
      "Epoch 52/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2684.4949 - mape: 19.0990 - mse: 23273686.0000 - val_loss: 2519.1650 - val_mape: 17.7361 - val_mse: 21848266.0000\n",
      "Epoch 53/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2678.9841 - mape: 19.0036 - mse: 23277846.0000 - val_loss: 2542.0461 - val_mape: 17.4216 - val_mse: 22393332.0000\n",
      "Epoch 54/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2675.8567 - mape: 18.9319 - mse: 23303416.0000 - val_loss: 2504.5522 - val_mape: 17.3386 - val_mse: 21872660.0000\n",
      "Epoch 55/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2668.7158 - mape: 18.8588 - mse: 23225326.0000 - val_loss: 2541.6907 - val_mape: 17.3816 - val_mse: 22244604.0000\n",
      "Epoch 56/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2665.0132 - mape: 18.7876 - mse: 23158336.0000 - val_loss: 2516.7529 - val_mape: 17.2670 - val_mse: 21961920.0000\n",
      "Epoch 57/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2654.9714 - mape: 18.7160 - mse: 23118950.0000 - val_loss: 2495.1189 - val_mape: 17.3809 - val_mse: 21865956.0000\n",
      "Epoch 58/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2652.2258 - mape: 18.6319 - mse: 23161100.0000 - val_loss: 2499.9414 - val_mape: 17.4022 - val_mse: 21728582.0000\n",
      "Epoch 59/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2649.3235 - mape: 18.6217 - mse: 23135442.0000 - val_loss: 2495.2769 - val_mape: 17.0191 - val_mse: 21678152.0000\n",
      "Epoch 60/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2645.9028 - mape: 18.5683 - mse: 23093814.0000 - val_loss: 2487.1172 - val_mape: 17.2225 - val_mse: 21793774.0000\n",
      "Epoch 61/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2642.2639 - mape: 18.5285 - mse: 23039476.0000 - val_loss: 2483.6160 - val_mape: 17.4823 - val_mse: 21825112.0000\n",
      "Epoch 62/100\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2640.7051 - mape: 18.4757 - mse: 23143932.0000 - val_loss: 2508.4009 - val_mape: 17.3670 - val_mse: 21851392.0000\n",
      "Epoch 63/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2630.0149 - mape: 18.3715 - mse: 22980642.0000 - val_loss: 2495.1377 - val_mape: 17.1863 - val_mse: 21990254.0000\n",
      "Epoch 64/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2628.9316 - mape: 18.3425 - mse: 22985168.0000 - val_loss: 2479.6445 - val_mape: 17.2092 - val_mse: 21803236.0000\n",
      "Epoch 65/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2626.2666 - mape: 18.2931 - mse: 22995518.0000 - val_loss: 2475.8679 - val_mape: 16.9530 - val_mse: 21515788.0000\n",
      "Epoch 66/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2621.4473 - mape: 18.2674 - mse: 22956364.0000 - val_loss: 2461.4241 - val_mape: 16.8286 - val_mse: 21648194.0000\n",
      "Epoch 67/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2619.4038 - mape: 18.2743 - mse: 22871108.0000 - val_loss: 2479.5471 - val_mape: 17.2400 - val_mse: 21517932.0000\n",
      "Epoch 68/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2614.2095 - mape: 18.2031 - mse: 22871816.0000 - val_loss: 2476.4658 - val_mape: 17.4310 - val_mse: 21358932.0000\n",
      "Epoch 69/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2611.7842 - mape: 18.1731 - mse: 22878720.0000 - val_loss: 2453.5933 - val_mape: 16.9324 - val_mse: 21495896.0000\n",
      "Epoch 70/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2610.1589 - mape: 18.1761 - mse: 22903316.0000 - val_loss: 2451.3638 - val_mape: 16.7811 - val_mse: 21534186.0000\n",
      "Epoch 71/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2608.2964 - mape: 18.1611 - mse: 22840896.0000 - val_loss: 2476.4854 - val_mape: 17.1081 - val_mse: 21886302.0000\n",
      "Epoch 72/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2601.9585 - mape: 18.1080 - mse: 22807242.0000 - val_loss: 2466.3608 - val_mape: 16.8047 - val_mse: 21827528.0000\n",
      "Epoch 73/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2601.8030 - mape: 18.1012 - mse: 22813636.0000 - val_loss: 2461.6201 - val_mape: 16.8403 - val_mse: 21559198.0000\n",
      "Epoch 74/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2596.7788 - mape: 18.0658 - mse: 22702372.0000 - val_loss: 2468.5581 - val_mape: 16.6473 - val_mse: 21737776.0000\n",
      "Epoch 75/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2594.4519 - mape: 18.0781 - mse: 22676386.0000 - val_loss: 2445.2002 - val_mape: 16.7662 - val_mse: 21611212.0000\n",
      "Epoch 76/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2592.1008 - mape: 17.9935 - mse: 22658568.0000 - val_loss: 2440.5408 - val_mape: 16.6338 - val_mse: 21527876.0000\n",
      "Epoch 77/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2591.9683 - mape: 18.0140 - mse: 22702464.0000 - val_loss: 2439.6746 - val_mape: 16.9590 - val_mse: 21346734.0000\n",
      "Epoch 78/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2587.7971 - mape: 18.0027 - mse: 22619538.0000 - val_loss: 2424.6216 - val_mape: 16.6310 - val_mse: 21343960.0000\n",
      "Epoch 79/100\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2586.4934 - mape: 17.9519 - mse: 22643644.0000 - val_loss: 2435.7419 - val_mape: 16.6240 - val_mse: 21592364.0000\n",
      "Epoch 80/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2584.6047 - mape: 17.9581 - mse: 22625530.0000 - val_loss: 2449.9246 - val_mape: 16.9136 - val_mse: 21194284.0000\n",
      "Epoch 81/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2582.7078 - mape: 17.9263 - mse: 22612134.0000 - val_loss: 2438.8137 - val_mape: 16.8045 - val_mse: 21391882.0000\n",
      "Epoch 82/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2580.9814 - mape: 17.9293 - mse: 22535140.0000 - val_loss: 2423.0864 - val_mape: 16.5929 - val_mse: 21446772.0000\n",
      "Epoch 83/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2578.7109 - mape: 17.8987 - mse: 22553844.0000 - val_loss: 2436.2163 - val_mape: 16.6032 - val_mse: 21368350.0000\n",
      "Epoch 84/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2577.0581 - mape: 17.8685 - mse: 22525640.0000 - val_loss: 2422.3481 - val_mape: 16.4885 - val_mse: 21241368.0000\n",
      "Epoch 85/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2573.9099 - mape: 17.8754 - mse: 22441782.0000 - val_loss: 2420.1174 - val_mape: 16.8983 - val_mse: 20988826.0000\n",
      "Epoch 86/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2572.9673 - mape: 17.8807 - mse: 22471630.0000 - val_loss: 2424.0891 - val_mape: 16.9025 - val_mse: 21210756.0000\n",
      "Epoch 87/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2572.5481 - mape: 17.8248 - mse: 22487790.0000 - val_loss: 2430.7107 - val_mape: 17.1512 - val_mse: 21016578.0000\n",
      "Epoch 88/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2567.1074 - mape: 17.8258 - mse: 22430078.0000 - val_loss: 2421.4526 - val_mape: 16.4766 - val_mse: 21134582.0000\n",
      "Epoch 89/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2568.6760 - mape: 17.8094 - mse: 22458488.0000 - val_loss: 2418.9136 - val_mape: 16.7756 - val_mse: 21344970.0000\n",
      "Epoch 90/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2563.7256 - mape: 17.8078 - mse: 22336024.0000 - val_loss: 2410.4404 - val_mape: 16.5704 - val_mse: 21330398.0000\n",
      "Epoch 91/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2563.1799 - mape: 17.8319 - mse: 22387358.0000 - val_loss: 2397.1147 - val_mape: 16.6753 - val_mse: 21103662.0000\n",
      "Epoch 92/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2563.1870 - mape: 17.7576 - mse: 22394940.0000 - val_loss: 2409.0933 - val_mape: 16.6780 - val_mse: 20937836.0000\n",
      "Epoch 93/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2558.7939 - mape: 17.7604 - mse: 22361322.0000 - val_loss: 2398.8921 - val_mape: 16.4532 - val_mse: 21179656.0000\n",
      "Epoch 94/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2557.2173 - mape: 17.7352 - mse: 22387032.0000 - val_loss: 2410.6765 - val_mape: 16.6710 - val_mse: 21184966.0000\n",
      "Epoch 95/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2557.2302 - mape: 17.7264 - mse: 22350464.0000 - val_loss: 2399.4346 - val_mape: 16.5073 - val_mse: 21142994.0000\n",
      "Epoch 96/100\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2555.9138 - mape: 17.7185 - mse: 22367606.0000 - val_loss: 2397.3235 - val_mape: 16.4135 - val_mse: 20786918.0000\n",
      "Epoch 97/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2554.0759 - mape: 17.6993 - mse: 22335788.0000 - val_loss: 2399.8987 - val_mape: 16.3858 - val_mse: 21300576.0000\n",
      "Epoch 98/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2556.1194 - mape: 17.7059 - mse: 22376730.0000 - val_loss: 2395.7463 - val_mape: 16.2284 - val_mse: 21328468.0000\n",
      "Epoch 99/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2547.6558 - mape: 17.6642 - mse: 22306118.0000 - val_loss: 2405.6450 - val_mape: 16.6883 - val_mse: 21070366.0000\n",
      "Epoch 100/100\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2548.3474 - mape: 17.6685 - mse: 22269842.0000 - val_loss: 2405.0886 - val_mape: 16.3345 - val_mse: 21281026.0000\n",
      "CPU times: user 48min 36s, sys: 9min 31s, total: 58min 8s\n",
      "Wall time: 30min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model on train set\n",
    "history = NN_baseline_model.fit(X_train_ss, y_train, epochs=100, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is complete. we can visualize the history in a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "gXaUQl4jwKaH",
    "outputId": "12b77c65-4780-4dcf-cf2f-5c9565b4f6e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc55869ba50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss vs epoch\n",
    "epochs = range(1, 101)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(epochs, history.history[\"loss\"], label=\"training\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], label=\"validation\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"mae\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Observations__\n",
    "- Losses of our model (MAE) kept minimizing for each epoch trained.\n",
    "- We observe diminishing returns after a certain number of epochs.\n",
    "- After 100 epochs our model has MAE $\\approx$ 2500 for both training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4__ Evaluate the model using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmzNMicf9gPw",
    "outputId": "aab859f4-2efa-4619-bd4b-398c10d1cefd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7507/7507 [==============================] - 10s 1ms/step\n",
      "1877/1877 [==============================] - 2s 1ms/step\n",
      "Model Performance\n",
      "-----------------\n",
      "Train set R2 score: 0.9582\n",
      "Test R2 score: 0.9581\n",
      "Test set Adjusted R2 score: 0.9581\n",
      "Test set Root Mean Squared Error - RMSE: 4638.6431\n",
      "Test set Mean Absolute Error - MAE: 2403.2708\n",
      "Test set Mean Absolute Precentage Error - MAPE: 0.1633\n",
      "CPU times: user 18.6 s, sys: 1.94 s, total: 20.5 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Score the model\n",
    "NN_baseline_model_performance = evaluate_NN(NN_baseline_model, X_test_ss, y_test, X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model performance is good with high R2 scores and no indication of overfitting. We have a MAE of 16.33% which is not good enough. (Goal is it have it below 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s improve our model by increasing the nodes and adding more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GxSic5CeAdbZ"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(123) # for consistent results\n",
    "\n",
    "# Create a new sequential model\n",
    "NN_model_1 = keras.Sequential()\n",
    "\n",
    "# Declare the input layer\n",
    "NN_model_1.add(layers.Dense(512, activation='relu', input_shape= (22,),  name='layer1'))\n",
    "\n",
    "# Add a dropout layer\n",
    "NN_model_1.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare hidden layers\n",
    "NN_model_1.add(layers.Dense(256, activation='relu',  name='layer2'))\n",
    "NN_model_1.add(layers.Dense(128, activation='relu',  name='layer3'))\n",
    "\n",
    "\n",
    "# Declare the output layer\n",
    "NN_model_1.add(layers.Dense(1, activation='relu', name='output')) # ReLU since price cannot have negative values\n",
    "\n",
    "# Compile the model\n",
    "NN_model_1.compile( \n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizer\n",
    "  loss=keras.losses.MeanAbsoluteError(), # Loss function to minimize\n",
    "  metrics=['mape', 'mse'] # metrics\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fRFWdR0oZwM",
    "outputId": "3db0720e-3612-40de-e554-00423e6ae443",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 512)               11776     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 256)               131328    \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 128)               32896     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176,129\n",
      "Trainable params: 176,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Looking at model structure\n",
    "NN_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vast number of trainable parameters (176,129) compared to the baseline model of 18,145 params. This should increase the performance. Since this will be a computationally intensive process, let’s first train for 20 epochs and inspect for undesirable behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-gTYuCxt0v5",
    "outputId": "6d882232-9143-4d27-bbdc-d9ea4277d783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6006/6006 [==============================] - 20s 3ms/step - loss: 5231.2329 - mape: 34.6677 - mse: 83823096.0000 - val_loss: 3899.1799 - val_mape: 24.8171 - val_mse: 43765064.0000\n",
      "Epoch 2/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 3564.0820 - mape: 24.1146 - mse: 35324348.0000 - val_loss: 2926.7478 - val_mape: 18.8475 - val_mse: 25417396.0000\n",
      "Epoch 3/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2980.8223 - mape: 20.0733 - mse: 26086488.0000 - val_loss: 2670.4138 - val_mape: 18.2613 - val_mse: 22381088.0000\n",
      "Epoch 4/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2812.1023 - mape: 18.6538 - mse: 24278104.0000 - val_loss: 2555.4634 - val_mape: 16.8204 - val_mse: 21496810.0000\n",
      "Epoch 5/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2719.2236 - mape: 17.9210 - mse: 23383910.0000 - val_loss: 2472.2458 - val_mape: 16.2448 - val_mse: 20827900.0000\n",
      "Epoch 6/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2653.0320 - mape: 17.5369 - mse: 22494444.0000 - val_loss: 2421.7173 - val_mape: 15.9436 - val_mse: 20316952.0000\n",
      "Epoch 7/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2603.6743 - mape: 17.1630 - mse: 22079006.0000 - val_loss: 2334.1150 - val_mape: 15.4076 - val_mse: 19493532.0000\n",
      "Epoch 8/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2566.3403 - mape: 16.8688 - mse: 21726546.0000 - val_loss: 2324.5571 - val_mape: 15.1980 - val_mse: 19424862.0000\n",
      "Epoch 9/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2526.8635 - mape: 16.6084 - mse: 21200326.0000 - val_loss: 2296.4314 - val_mape: 15.1053 - val_mse: 18885950.0000\n",
      "Epoch 10/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2499.0303 - mape: 16.3917 - mse: 20885074.0000 - val_loss: 2368.0049 - val_mape: 14.6050 - val_mse: 20330736.0000\n",
      "Epoch 11/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2469.7783 - mape: 16.2509 - mse: 20563098.0000 - val_loss: 2221.3274 - val_mape: 14.2291 - val_mse: 18397150.0000\n",
      "Epoch 12/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2446.4888 - mape: 16.0067 - mse: 20411190.0000 - val_loss: 2209.9099 - val_mape: 14.0530 - val_mse: 18108354.0000\n",
      "Epoch 13/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2418.1248 - mape: 15.8041 - mse: 20001198.0000 - val_loss: 2223.7358 - val_mape: 13.7972 - val_mse: 18733136.0000\n",
      "Epoch 14/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2402.6304 - mape: 15.6797 - mse: 19862762.0000 - val_loss: 2195.9897 - val_mape: 15.2147 - val_mse: 17583044.0000\n",
      "Epoch 15/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2374.8113 - mape: 15.5551 - mse: 19562826.0000 - val_loss: 2190.3970 - val_mape: 16.3145 - val_mse: 17233580.0000\n",
      "Epoch 16/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2355.5879 - mape: 15.4242 - mse: 19254312.0000 - val_loss: 2130.9956 - val_mape: 15.4686 - val_mse: 16945280.0000\n",
      "Epoch 17/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2334.7358 - mape: 15.2232 - mse: 19032452.0000 - val_loss: 2108.3596 - val_mape: 14.0800 - val_mse: 17310006.0000\n",
      "Epoch 18/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2309.0193 - mape: 15.0951 - mse: 18710254.0000 - val_loss: 2127.3564 - val_mape: 13.3073 - val_mse: 17387728.0000\n",
      "Epoch 19/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2289.9585 - mape: 14.9375 - mse: 18424306.0000 - val_loss: 2067.0842 - val_mape: 14.8003 - val_mse: 16267633.0000\n",
      "Epoch 20/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2274.3674 - mape: 14.8403 - mse: 18290158.0000 - val_loss: 2036.4221 - val_mape: 14.6084 - val_mse: 16372914.0000\n",
      "CPU times: user 9min 53s, sys: 1min 56s, total: 11min 50s\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model on train set\n",
    "history_1 = NN_model_1.fit(X_train_ss, y_train, epochs=20, validation_split=0.2, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe training loss is higher than the validation loss which is a sign of underfitting. This may be due to the dropout layer we added in order to prevent overfitting. We can reduce the number of dropouts or remove the dropout layer to reduce underfitting. Another reason can be the val loss is calculated at the end of an epoch and train loss is calculated on average, at half an epoch during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s remove the dropout layer and train the model for 20 epochs to see if it improved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LsKMBmbKuCbs"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(123) # for consistent results\n",
    "\n",
    "# Create a new sequential model\n",
    "NN_model_2 = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "NN_model_2.add(layers.Dense(512, activation='relu', input_shape= (22,),  name='layer1'))\n",
    "\n",
    "NN_model_2.add(layers.Dense(256, activation='relu',  name='layer2'))\n",
    "\n",
    "NN_model_2.add(layers.Dense(128, activation='relu',  name='layer3'))\n",
    "\n",
    "\n",
    "# Declare the output layer\n",
    "NN_model_2.add(layers.Dense(1, activation='relu', name='output')) # ReLU since price cannot have negative values\n",
    "\n",
    "# Compile the model\n",
    "NN_model_2.compile( \n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizer\n",
    "  loss=keras.losses.MeanAbsoluteError(), # Loss function to minimize\n",
    "  metrics=['mape', 'mse'] # metrics\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d6XAuTDvhvo",
    "outputId": "afa6b996-30b7-482a-bbe2-481849a5e066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 4849.7427 - mape: 32.2375 - mse: 75521640.0000 - val_loss: 3176.5156 - val_mape: 21.4043 - val_mse: 29155808.0000\n",
      "Epoch 2/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2904.2742 - mape: 20.1978 - mse: 25124092.0000 - val_loss: 2693.1582 - val_mape: 18.3811 - val_mse: 23001872.0000\n",
      "Epoch 3/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2623.0564 - mape: 18.3667 - mse: 22064052.0000 - val_loss: 2544.1821 - val_mape: 18.2963 - val_mse: 20649852.0000\n",
      "Epoch 4/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2483.4817 - mape: 17.3006 - mse: 20775316.0000 - val_loss: 2535.6499 - val_mape: 16.6183 - val_mse: 21098692.0000\n",
      "Epoch 5/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2388.8943 - mape: 16.4586 - mse: 19959116.0000 - val_loss: 2300.2046 - val_mape: 15.9183 - val_mse: 19128014.0000\n",
      "Epoch 6/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2312.7913 - mape: 15.8336 - mse: 19258770.0000 - val_loss: 2240.5498 - val_mape: 15.4151 - val_mse: 18475036.0000\n",
      "Epoch 7/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2252.4390 - mape: 15.3619 - mse: 18598530.0000 - val_loss: 2279.8071 - val_mape: 15.3552 - val_mse: 19041264.0000\n",
      "Epoch 8/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2201.5330 - mape: 15.0498 - mse: 18092144.0000 - val_loss: 2190.7705 - val_mape: 15.9194 - val_mse: 17515880.0000\n",
      "Epoch 9/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2166.5552 - mape: 14.8277 - mse: 17624602.0000 - val_loss: 2151.0625 - val_mape: 14.0309 - val_mse: 17985178.0000\n",
      "Epoch 10/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2126.8755 - mape: 14.5671 - mse: 17298610.0000 - val_loss: 2136.8569 - val_mape: 14.3117 - val_mse: 16811724.0000\n",
      "Epoch 11/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2092.8315 - mape: 14.3334 - mse: 16904486.0000 - val_loss: 2191.8320 - val_mape: 16.1784 - val_mse: 16526064.0000\n",
      "Epoch 12/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2072.7556 - mape: 14.2065 - mse: 16703564.0000 - val_loss: 2023.0529 - val_mape: 13.7175 - val_mse: 16407104.0000\n",
      "Epoch 13/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2044.8495 - mape: 14.0609 - mse: 16330617.0000 - val_loss: 2092.0579 - val_mape: 13.6509 - val_mse: 17152666.0000\n",
      "Epoch 14/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 2016.4724 - mape: 13.8428 - mse: 16015599.0000 - val_loss: 1976.7635 - val_mape: 13.5897 - val_mse: 15807292.0000\n",
      "Epoch 15/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 2002.5889 - mape: 13.7544 - mse: 15880527.0000 - val_loss: 1994.7124 - val_mape: 14.8523 - val_mse: 15251958.0000\n",
      "Epoch 16/20\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1981.2148 - mape: 13.6429 - mse: 15612501.0000 - val_loss: 1990.4457 - val_mape: 14.2872 - val_mse: 15300303.0000\n",
      "Epoch 17/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 1964.1129 - mape: 13.5000 - mse: 15369465.0000 - val_loss: 2088.9788 - val_mape: 14.1109 - val_mse: 15420736.0000\n",
      "Epoch 18/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 1943.9264 - mape: 13.3927 - mse: 15211729.0000 - val_loss: 1934.6758 - val_mape: 13.3676 - val_mse: 15451062.0000\n",
      "Epoch 19/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 1926.2367 - mape: 13.2732 - mse: 14962760.0000 - val_loss: 1913.1976 - val_mape: 13.4879 - val_mse: 14559529.0000\n",
      "Epoch 20/20\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 1905.7051 - mape: 13.1589 - mse: 14765711.0000 - val_loss: 1940.6168 - val_mape: 15.4264 - val_mse: 14554236.0000\n",
      "CPU times: user 9min 44s, sys: 1min 58s, total: 11min 42s\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_2 = NN_model_2.fit(X_train_ss, y_train, epochs=20, validation_split=0.2, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation losses are approximately similar after removing the dropout layer. We will continue to train the model without dropout layers as it seems unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are objects in Keras called 'Callbacks' that will perform various acts during the training stage. Let’s employ them to efficiently train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EarlyStopping`\n",
    "\n",
    "NN training is a demanding time-consuming process as the number of epochs increases. Early stopping will monitor the desired metric ('val_loss' for our model) and terminate training when there is no significant improvement.\n",
    "\n",
    "Importance parameters of __EarlyStopping__;\n",
    "- __*monitor*__ - Metric to be monitored\n",
    "- __*mode*__ - Tells if the metric monitored should decrease or increase to terminate training. 'min' for regression, 'max' for classification\n",
    "- __*patience*__ - Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReduceLROnPlateau`\n",
    "\n",
    "Models often improve with low learning rates. This callback monitors the improvement of the metric and reduces the learning rate if there is no improvement seen for a predefined number of epochs.\n",
    "\n",
    "\n",
    "Importance parameters of __ReduceLROnPlateau__;\n",
    "- __*monitor*__ - Metric to be monitored.\n",
    "- __*factor*__ - the amount by which the learning rate will reduce if there is no improvement.\n",
    "- __*mode*__ - Tells if the metric monitored should decrease or increase to consider reducing learning rate. 'min' for regression, 'max' for classification\n",
    "- __*patience*__ - Number of epochs with no improvement after which the learning rate will reduce.\n",
    "- __*min_delta*__ - Threshold value. If there is no improvement larger than threshold for number of epochs specified at 'patience', the learning rate will be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelChekpoint`\n",
    "\n",
    "Save the model weights at an interval such that the model and weights can be loaded later to continue training.\n",
    "\n",
    "Importance parameters of __ModelChekpoint__;\n",
    "- __*monitor*__ - Metric to be monitored\n",
    "- __*mode*__ - Tells if the metric monitored should decrease or increase to be considered as the best model. 'min' for regression, 'max' for classification\n",
    "- __*save_best_only*__ -Boolean value. If True, only saves the model when it is considered best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "WmxbHT8JAdeS"
   },
   "outputs": [],
   "source": [
    "# Define Early stopping callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "L0Ox5gSAAdhK"
   },
   "outputs": [],
   "source": [
    "# Define learning rate reducer callback\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Y2oHksPpAdjz"
   },
   "outputs": [],
   "source": [
    "# Definre model checkpoint callback\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZ7QVq4SAdmW",
    "outputId": "e7ffcbc2-657c-4c50-fd62-50d9ab174446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1886.6660 - mape: 13.0527 - mse: 14547877.0000\n",
      "Epoch 1: val_loss improved from inf to 1883.53870, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1886.9519 - mape: 13.0523 - mse: 14555265.0000 - val_loss: 1883.5387 - val_mape: 13.0702 - val_mse: 14230620.0000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1873.2155 - mape: 12.9301 - mse: 14411472.0000\n",
      "Epoch 2: val_loss improved from 1883.53870 to 1857.26001, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1872.4855 - mape: 12.9278 - mse: 14400681.0000 - val_loss: 1857.2600 - val_mape: 12.8565 - val_mse: 14207587.0000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1855.7560 - mape: 12.8165 - mse: 14172805.0000\n",
      "Epoch 3: val_loss did not improve from 1857.26001\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1855.8040 - mape: 12.8150 - mse: 14172093.0000 - val_loss: 1928.7372 - val_mape: 14.1684 - val_mse: 14215288.0000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1841.3660 - mape: 12.7356 - mse: 13938511.0000\n",
      "Epoch 4: val_loss did not improve from 1857.26001\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1841.5375 - mape: 12.7367 - mse: 13942536.0000 - val_loss: 1885.7428 - val_mape: 12.6708 - val_mse: 14870175.0000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1826.4755 - mape: 12.6191 - mse: 13797655.0000\n",
      "Epoch 5: val_loss improved from 1857.26001 to 1814.46252, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1826.4446 - mape: 12.6190 - mse: 13797320.0000 - val_loss: 1814.4625 - val_mape: 12.7495 - val_mse: 13804121.0000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 1805.2401 - mape: 12.4548 - mse: 13596558.0000\n",
      "Epoch 6: val_loss improved from 1814.46252 to 1773.00256, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1805.0046 - mape: 12.4543 - mse: 13589934.0000 - val_loss: 1773.0026 - val_mape: 12.2556 - val_mse: 13374967.0000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1783.8492 - mape: 12.3032 - mse: 13315421.0000\n",
      "Epoch 7: val_loss did not improve from 1773.00256\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1784.1788 - mape: 12.3034 - mse: 13317930.0000 - val_loss: 1803.0720 - val_mape: 12.6386 - val_mse: 13491582.0000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1770.0035 - mape: 12.2063 - mse: 13150388.0000\n",
      "Epoch 8: val_loss did not improve from 1773.00256\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1769.8590 - mape: 12.2045 - mse: 13150985.0000 - val_loss: 1782.7900 - val_mape: 12.0972 - val_mse: 13053473.0000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 1749.2484 - mape: 12.0685 - mse: 12869155.0000\n",
      "Epoch 9: val_loss did not improve from 1773.00256\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1749.2783 - mape: 12.0662 - mse: 12873085.0000 - val_loss: 1800.9662 - val_mape: 11.9739 - val_mse: 13965049.0000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1734.0215 - mape: 11.9325 - mse: 12736523.0000\n",
      "Epoch 10: val_loss improved from 1773.00256 to 1688.14282, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1734.0215 - mape: 11.9325 - mse: 12736523.0000 - val_loss: 1688.1428 - val_mape: 11.8602 - val_mse: 12375334.0000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1713.8954 - mape: 11.8016 - mse: 12502524.0000\n",
      "Epoch 11: val_loss did not improve from 1688.14282\n",
      "6006/6006 [==============================] - 18s 3ms/step - loss: 1713.9913 - mape: 11.8003 - mse: 12506807.0000 - val_loss: 1751.8103 - val_mape: 11.8557 - val_mse: 12805696.0000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1703.6812 - mape: 11.6711 - mse: 12386111.0000\n",
      "Epoch 12: val_loss did not improve from 1688.14282\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1703.3480 - mape: 11.6701 - mse: 12382952.0000 - val_loss: 1719.2444 - val_mape: 11.5021 - val_mse: 12916877.0000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1685.5598 - mape: 11.5435 - mse: 12207675.0000\n",
      "Epoch 13: val_loss did not improve from 1688.14282\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1685.5575 - mape: 11.5434 - mse: 12207544.0000 - val_loss: 1733.9481 - val_mape: 11.4068 - val_mse: 12983556.0000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1667.5380 - mape: 11.4214 - mse: 12014677.0000\n",
      "Epoch 14: val_loss did not improve from 1688.14282\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1667.5826 - mape: 11.4212 - mse: 12014227.0000 - val_loss: 1717.4303 - val_mape: 12.0113 - val_mse: 12036281.0000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1653.6919 - mape: 11.3103 - mse: 11938730.0000\n",
      "Epoch 15: val_loss improved from 1688.14282 to 1669.52930, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1653.6835 - mape: 11.3102 - mse: 11938467.0000 - val_loss: 1669.5293 - val_mape: 11.6799 - val_mse: 12185673.0000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 1641.8591 - mape: 11.2067 - mse: 11746073.0000\n",
      "Epoch 16: val_loss improved from 1669.52930 to 1665.14954, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1641.7460 - mape: 11.2077 - mse: 11743394.0000 - val_loss: 1665.1495 - val_mape: 11.6129 - val_mse: 11780640.0000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1631.1481 - mape: 11.1089 - mse: 11634100.0000\n",
      "Epoch 17: val_loss improved from 1665.14954 to 1615.16833, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1631.2473 - mape: 11.1083 - mse: 11632893.0000 - val_loss: 1615.1683 - val_mape: 11.0292 - val_mse: 11456628.0000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1619.1661 - mape: 11.0167 - mse: 11555988.0000\n",
      "Epoch 18: val_loss did not improve from 1615.16833\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1619.4998 - mape: 11.0174 - mse: 11562277.0000 - val_loss: 1632.2534 - val_mape: 11.1861 - val_mse: 11814448.0000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1605.7649 - mape: 10.9073 - mse: 11385546.0000\n",
      "Epoch 19: val_loss improved from 1615.16833 to 1597.72949, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1605.6107 - mape: 10.9069 - mse: 11385612.0000 - val_loss: 1597.7295 - val_mape: 11.1342 - val_mse: 11147638.0000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1593.9224 - mape: 10.8308 - mse: 11313665.0000\n",
      "Epoch 20: val_loss did not improve from 1597.72949\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1593.8999 - mape: 10.8308 - mse: 11313399.0000 - val_loss: 1642.6139 - val_mape: 11.9342 - val_mse: 11424153.0000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1582.7910 - mape: 10.7314 - mse: 11224082.0000\n",
      "Epoch 21: val_loss did not improve from 1597.72949\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1583.0739 - mape: 10.7315 - mse: 11226352.0000 - val_loss: 1598.2468 - val_mape: 10.7276 - val_mse: 11064035.0000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 1572.6067 - mape: 10.6439 - mse: 11083561.0000\n",
      "Epoch 22: val_loss improved from 1597.72949 to 1580.63428, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1572.8105 - mape: 10.6467 - mse: 11087381.0000 - val_loss: 1580.6343 - val_mape: 10.9203 - val_mse: 11095267.0000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1565.2156 - mape: 10.6055 - mse: 10996188.0000\n",
      "Epoch 23: val_loss improved from 1580.63428 to 1574.77039, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1565.4652 - mape: 10.6065 - mse: 11003104.0000 - val_loss: 1574.7704 - val_mape: 10.6568 - val_mse: 10827812.0000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1553.5614 - mape: 10.5190 - mse: 10875619.0000\n",
      "Epoch 24: val_loss improved from 1574.77039 to 1554.86169, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1553.7332 - mape: 10.5184 - mse: 10876806.0000 - val_loss: 1554.8617 - val_mape: 10.7730 - val_mse: 10961355.0000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1543.8945 - mape: 10.4326 - mse: 10836224.0000\n",
      "Epoch 25: val_loss did not improve from 1554.86169\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1543.7970 - mape: 10.4331 - mse: 10834287.0000 - val_loss: 1573.6089 - val_mape: 10.6912 - val_mse: 10712452.0000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1537.0950 - mape: 10.3662 - mse: 10759278.0000\n",
      "Epoch 26: val_loss did not improve from 1554.86169\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1537.2007 - mape: 10.3671 - mse: 10761140.0000 - val_loss: 1689.2026 - val_mape: 12.0946 - val_mse: 10804304.0000 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "5987/6006 [============================>.] - ETA: 0s - loss: 1528.1971 - mape: 10.2865 - mse: 10702458.0000\n",
      "Epoch 27: val_loss improved from 1554.86169 to 1518.29700, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1528.3192 - mape: 10.2869 - mse: 10700257.0000 - val_loss: 1518.2970 - val_mape: 10.3513 - val_mse: 10792895.0000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1519.8805 - mape: 10.2362 - mse: 10569385.0000\n",
      "Epoch 28: val_loss improved from 1518.29700 to 1512.16602, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1519.7419 - mape: 10.2354 - mse: 10566913.0000 - val_loss: 1512.1660 - val_mape: 10.1095 - val_mse: 10680408.0000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1514.0045 - mape: 10.1747 - mse: 10499233.0000\n",
      "Epoch 29: val_loss did not improve from 1512.16602\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1514.1874 - mape: 10.1755 - mse: 10499653.0000 - val_loss: 1549.4176 - val_mape: 10.2468 - val_mse: 10934022.0000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 1506.6121 - mape: 10.0908 - mse: 10503410.0000\n",
      "Epoch 30: val_loss did not improve from 1512.16602\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1506.1331 - mape: 10.0920 - mse: 10494712.0000 - val_loss: 1519.6642 - val_mape: 10.3421 - val_mse: 10849163.0000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1491.7490 - mape: 10.0052 - mse: 10358192.0000\n",
      "Epoch 31: val_loss improved from 1512.16602 to 1504.64697, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1491.6632 - mape: 10.0049 - mse: 10357303.0000 - val_loss: 1504.6470 - val_mape: 10.2961 - val_mse: 10650341.0000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1484.7174 - mape: 9.9651 - mse: 10285286.0000\n",
      "Epoch 32: val_loss improved from 1504.64697 to 1500.11438, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1484.7042 - mape: 9.9651 - mse: 10285058.0000 - val_loss: 1500.1144 - val_mape: 10.3258 - val_mse: 10263819.0000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1481.0439 - mape: 9.9045 - mse: 10245187.0000\n",
      "Epoch 33: val_loss did not improve from 1500.11438\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1481.0439 - mape: 9.9045 - mse: 10245187.0000 - val_loss: 1503.9386 - val_mape: 10.0150 - val_mse: 10959381.0000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1470.9333 - mape: 9.8431 - mse: 10151620.0000\n",
      "Epoch 34: val_loss did not improve from 1500.11438\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1470.7972 - mape: 9.8410 - mse: 10151276.0000 - val_loss: 1518.8179 - val_mape: 10.0636 - val_mse: 11098519.0000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 1461.2440 - mape: 9.7667 - mse: 10084430.0000\n",
      "Epoch 35: val_loss did not improve from 1500.11438\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1461.2549 - mape: 9.7672 - mse: 10084661.0000 - val_loss: 1547.0648 - val_mape: 9.9820 - val_mse: 11102104.0000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1453.5756 - mape: 9.7005 - mse: 10028360.0000\n",
      "Epoch 36: val_loss improved from 1500.11438 to 1470.71814, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1454.1891 - mape: 9.7012 - mse: 10038395.0000 - val_loss: 1470.7181 - val_mape: 9.9276 - val_mse: 10337221.0000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1447.1842 - mape: 9.6408 - mse: 9970676.0000\n",
      "Epoch 37: val_loss did not improve from 1470.71814\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1447.1842 - mape: 9.6408 - mse: 9970676.0000 - val_loss: 1523.3903 - val_mape: 9.9290 - val_mse: 10297128.0000 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 1440.4425 - mape: 9.6204 - mse: 9902851.0000\n",
      "Epoch 38: val_loss improved from 1470.71814 to 1459.02783, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1440.5701 - mape: 9.6224 - mse: 9903106.0000 - val_loss: 1459.0278 - val_mape: 10.0130 - val_mse: 10025631.0000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1433.7611 - mape: 9.5662 - mse: 9842677.0000\n",
      "Epoch 39: val_loss did not improve from 1459.02783\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1434.2291 - mape: 9.5675 - mse: 9850467.0000 - val_loss: 1465.0743 - val_mape: 9.6642 - val_mse: 10428974.0000 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1422.5173 - mape: 9.4670 - mse: 9789486.0000\n",
      "Epoch 40: val_loss did not improve from 1459.02783\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1422.5654 - mape: 9.4688 - mse: 9787663.0000 - val_loss: 1501.1245 - val_mape: 10.2778 - val_mse: 9881398.0000 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1416.6915 - mape: 9.4395 - mse: 9695189.0000\n",
      "Epoch 41: val_loss improved from 1459.02783 to 1441.61438, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1417.0125 - mape: 9.4408 - mse: 9704118.0000 - val_loss: 1441.6144 - val_mape: 9.8124 - val_mse: 9808122.0000 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1409.0953 - mape: 9.3741 - mse: 9659721.0000\n",
      "Epoch 42: val_loss did not improve from 1441.61438\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1408.7834 - mape: 9.3727 - mse: 9655543.0000 - val_loss: 1469.4244 - val_mape: 9.6441 - val_mse: 10567467.0000 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1408.8199 - mape: 9.3616 - mse: 9618939.0000\n",
      "Epoch 43: val_loss improved from 1441.61438 to 1406.21960, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1409.0306 - mape: 9.3592 - mse: 9622094.0000 - val_loss: 1406.2196 - val_mape: 9.6603 - val_mse: 9615144.0000 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1400.8185 - mape: 9.2974 - mse: 9594997.0000\n",
      "Epoch 44: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1400.8185 - mape: 9.2974 - mse: 9594997.0000 - val_loss: 1480.0065 - val_mape: 9.5959 - val_mse: 10657227.0000 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 1395.7017 - mape: 9.2722 - mse: 9551722.0000\n",
      "Epoch 45: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1395.6567 - mape: 9.2710 - mse: 9553596.0000 - val_loss: 1476.6141 - val_mape: 9.6512 - val_mse: 10629644.0000 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1387.9792 - mape: 9.2088 - mse: 9469403.0000\n",
      "Epoch 46: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1387.7039 - mape: 9.2079 - mse: 9464658.0000 - val_loss: 1415.1279 - val_mape: 9.5108 - val_mse: 9850835.0000 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1379.2635 - mape: 9.1734 - mse: 9414381.0000\n",
      "Epoch 47: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1379.0804 - mape: 9.1733 - mse: 9410030.0000 - val_loss: 1412.8824 - val_mape: 9.4574 - val_mse: 9923440.0000 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1372.8313 - mape: 9.1033 - mse: 9342170.0000\n",
      "Epoch 48: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1372.6851 - mape: 9.1016 - mse: 9341754.0000 - val_loss: 1454.3047 - val_mape: 10.6105 - val_mse: 9296700.0000 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1368.6743 - mape: 9.0697 - mse: 9304543.0000\n",
      "Epoch 49: val_loss did not improve from 1406.21960\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1368.6743 - mape: 9.0697 - mse: 9304543.0000 - val_loss: 1439.5441 - val_mape: 9.4235 - val_mse: 9732156.0000 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 1359.4342 - mape: 9.0258 - mse: 9266071.0000\n",
      "Epoch 50: val_loss improved from 1406.21960 to 1405.15686, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1359.3887 - mape: 9.0274 - mse: 9263903.0000 - val_loss: 1405.1569 - val_mape: 9.6689 - val_mse: 9829969.0000 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1354.6171 - mape: 8.9674 - mse: 9216524.0000\n",
      "Epoch 51: val_loss did not improve from 1405.15686\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1354.8024 - mape: 8.9673 - mse: 9219929.0000 - val_loss: 1429.8529 - val_mape: 9.9644 - val_mse: 9608714.0000 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1348.3987 - mape: 8.9236 - mse: 9125839.0000\n",
      "Epoch 52: val_loss improved from 1405.15686 to 1391.44519, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1349.3064 - mape: 8.9255 - mse: 9141078.0000 - val_loss: 1391.4452 - val_mape: 9.2826 - val_mse: 9474720.0000 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1341.3290 - mape: 8.8823 - mse: 9114867.0000\n",
      "Epoch 53: val_loss did not improve from 1391.44519\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1341.5560 - mape: 8.8829 - mse: 9114046.0000 - val_loss: 1499.0219 - val_mape: 9.4336 - val_mse: 10676714.0000 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1337.0435 - mape: 8.8377 - mse: 9090878.0000\n",
      "Epoch 54: val_loss did not improve from 1391.44519\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1337.2416 - mape: 8.8388 - mse: 9094025.0000 - val_loss: 1396.4626 - val_mape: 9.4427 - val_mse: 9768870.0000 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1331.6595 - mape: 8.8161 - mse: 8997142.0000\n",
      "Epoch 55: val_loss improved from 1391.44519 to 1385.88220, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1331.7698 - mape: 8.8180 - mse: 8996025.0000 - val_loss: 1385.8822 - val_mape: 9.2688 - val_mse: 9782319.0000 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1321.6451 - mape: 8.7345 - mse: 8939272.0000\n",
      "Epoch 56: val_loss improved from 1385.88220 to 1354.96436, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1321.2450 - mape: 8.7341 - mse: 8933941.0000 - val_loss: 1354.9644 - val_mape: 8.9130 - val_mse: 9366206.0000 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 1319.3888 - mape: 8.7126 - mse: 8940179.0000\n",
      "Epoch 57: val_loss did not improve from 1354.96436\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1319.4131 - mape: 8.7122 - mse: 8944137.0000 - val_loss: 1367.8616 - val_mape: 9.2304 - val_mse: 9172767.0000 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1312.9017 - mape: 8.6732 - mse: 8882899.0000\n",
      "Epoch 58: val_loss improved from 1354.96436 to 1354.19080, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1313.0844 - mape: 8.6724 - mse: 8891983.0000 - val_loss: 1354.1908 - val_mape: 9.0169 - val_mse: 9083216.0000 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1308.4315 - mape: 8.6257 - mse: 8852527.0000\n",
      "Epoch 59: val_loss did not improve from 1354.19080\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1308.2905 - mape: 8.6261 - mse: 8847896.0000 - val_loss: 1436.5745 - val_mape: 9.1987 - val_mse: 10074738.0000 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1304.4540 - mape: 8.6130 - mse: 8816412.0000\n",
      "Epoch 60: val_loss did not improve from 1354.19080\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1304.5411 - mape: 8.6137 - mse: 8813251.0000 - val_loss: 1435.9103 - val_mape: 9.6564 - val_mse: 8902016.0000 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1296.2228 - mape: 8.5844 - mse: 8726277.0000\n",
      "Epoch 61: val_loss improved from 1354.19080 to 1316.72449, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1296.4886 - mape: 8.5843 - mse: 8731982.0000 - val_loss: 1316.7245 - val_mape: 8.8552 - val_mse: 8980811.0000 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 1291.8484 - mape: 8.5281 - mse: 8680946.0000\n",
      "Epoch 62: val_loss did not improve from 1316.72449\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1291.8602 - mape: 8.5287 - mse: 8680745.0000 - val_loss: 1363.3226 - val_mape: 9.4673 - val_mse: 8908811.0000 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1290.1852 - mape: 8.4961 - mse: 8678145.0000\n",
      "Epoch 63: val_loss did not improve from 1316.72449\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1289.8600 - mape: 8.4971 - mse: 8677678.0000 - val_loss: 1318.0488 - val_mape: 8.8872 - val_mse: 9002631.0000 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1280.0928 - mape: 8.4461 - mse: 8632635.0000\n",
      "Epoch 64: val_loss did not improve from 1316.72449\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1279.9843 - mape: 8.4461 - mse: 8630626.0000 - val_loss: 1372.9877 - val_mape: 8.8756 - val_mse: 9719752.0000 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1281.6584 - mape: 8.4274 - mse: 8621919.0000\n",
      "Epoch 65: val_loss did not improve from 1316.72449\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1281.6244 - mape: 8.4271 - mse: 8621723.0000 - val_loss: 1320.3868 - val_mape: 9.0281 - val_mse: 8899056.0000 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1271.4862 - mape: 8.3762 - mse: 8581428.0000\n",
      "Epoch 66: val_loss did not improve from 1316.72449\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1271.5660 - mape: 8.3764 - mse: 8581154.0000 - val_loss: 1336.3049 - val_mape: 8.8998 - val_mse: 9184032.0000 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 1268.2881 - mape: 8.3535 - mse: 8535555.0000\n",
      "Epoch 67: val_loss improved from 1316.72449 to 1316.39087, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1268.1678 - mape: 8.3511 - mse: 8535087.0000 - val_loss: 1316.3909 - val_mape: 8.8509 - val_mse: 8706094.0000 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1264.4210 - mape: 8.3449 - mse: 8513690.0000\n",
      "Epoch 68: val_loss did not improve from 1316.39087\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1264.6196 - mape: 8.3453 - mse: 8513346.0000 - val_loss: 1356.9695 - val_mape: 9.0755 - val_mse: 8730771.0000 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1259.8042 - mape: 8.3024 - mse: 8472431.0000\n",
      "Epoch 69: val_loss did not improve from 1316.39087\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1259.5239 - mape: 8.3003 - mse: 8468731.0000 - val_loss: 1326.4115 - val_mape: 9.0899 - val_mse: 8827192.0000 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1258.4032 - mape: 8.2821 - mse: 8482419.0000\n",
      "Epoch 70: val_loss did not improve from 1316.39087\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1258.2334 - mape: 8.2816 - mse: 8484129.0000 - val_loss: 1316.7214 - val_mape: 8.8656 - val_mse: 9328274.0000 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1250.6517 - mape: 8.2440 - mse: 8426381.0000\n",
      "Epoch 71: val_loss did not improve from 1316.39087\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1250.4889 - mape: 8.2431 - mse: 8424002.0000 - val_loss: 1322.3212 - val_mape: 8.7154 - val_mse: 8803753.0000 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1244.9650 - mape: 8.2211 - mse: 8368128.5000\n",
      "Epoch 72: val_loss did not improve from 1316.39087\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1244.6232 - mape: 8.2199 - mse: 8362438.5000 - val_loss: 1316.9568 - val_mape: 8.7474 - val_mse: 9064230.0000 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1240.0757 - mape: 8.1591 - mse: 8389097.0000\n",
      "Epoch 73: val_loss improved from 1316.39087 to 1302.50049, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1240.4607 - mape: 8.1592 - mse: 8394751.0000 - val_loss: 1302.5005 - val_mape: 8.5675 - val_mse: 8978176.0000 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1237.7225 - mape: 8.1419 - mse: 8366229.0000\n",
      "Epoch 74: val_loss improved from 1302.50049 to 1268.16003, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1237.7722 - mape: 8.1421 - mse: 8366266.5000 - val_loss: 1268.1600 - val_mape: 8.5388 - val_mse: 8912185.0000 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 1231.9814 - mape: 8.1104 - mse: 8298666.0000\n",
      "Epoch 75: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1232.0371 - mape: 8.1104 - mse: 8299468.5000 - val_loss: 1315.1549 - val_mape: 8.9293 - val_mse: 9115554.0000 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1227.9948 - mape: 8.0671 - mse: 8276610.0000\n",
      "Epoch 76: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1227.7477 - mape: 8.0658 - mse: 8276653.5000 - val_loss: 1303.9409 - val_mape: 8.7657 - val_mse: 9305765.0000 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1223.0508 - mape: 8.0450 - mse: 8238746.5000\n",
      "Epoch 77: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1223.2120 - mape: 8.0447 - mse: 8241641.5000 - val_loss: 1275.5608 - val_mape: 8.6262 - val_mse: 8775973.0000 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1219.4504 - mape: 8.0197 - mse: 8211648.5000\n",
      "Epoch 78: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1219.4504 - mape: 8.0197 - mse: 8211648.5000 - val_loss: 1291.3638 - val_mape: 8.8744 - val_mse: 8675951.0000 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1213.8408 - mape: 8.0078 - mse: 8178486.0000\n",
      "Epoch 79: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1213.8625 - mape: 8.0083 - mse: 8176621.0000 - val_loss: 1278.1202 - val_mape: 8.5418 - val_mse: 9127354.0000 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1211.1149 - mape: 7.9628 - mse: 8151122.0000\n",
      "Epoch 80: val_loss did not improve from 1268.16003\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1210.9275 - mape: 7.9627 - mse: 8145421.5000 - val_loss: 1354.6346 - val_mape: 8.8160 - val_mse: 9428797.0000 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1207.8939 - mape: 7.9369 - mse: 8160955.5000\n",
      "Epoch 81: val_loss improved from 1268.16003 to 1266.30554, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1207.7808 - mape: 7.9371 - mse: 8159123.5000 - val_loss: 1266.3055 - val_mape: 8.4688 - val_mse: 8520544.0000 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 1205.1407 - mape: 7.9201 - mse: 8135970.0000\n",
      "Epoch 82: val_loss did not improve from 1266.30554\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1205.3647 - mape: 7.9204 - mse: 8139710.5000 - val_loss: 1331.1869 - val_mape: 8.6181 - val_mse: 9181677.0000 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1201.3005 - mape: 7.8992 - mse: 8091842.5000\n",
      "Epoch 83: val_loss improved from 1266.30554 to 1251.93970, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1201.2443 - mape: 7.8989 - mse: 8091056.5000 - val_loss: 1251.9397 - val_mape: 8.4632 - val_mse: 8796199.0000 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1198.8029 - mape: 7.8755 - mse: 8115020.0000\n",
      "Epoch 84: val_loss did not improve from 1251.93970\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1198.7952 - mape: 7.8757 - mse: 8114843.5000 - val_loss: 1286.6980 - val_mape: 8.4646 - val_mse: 9150259.0000 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 1194.8579 - mape: 7.8328 - mse: 8049104.5000\n",
      "Epoch 85: val_loss did not improve from 1251.93970\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1195.1365 - mape: 7.8330 - mse: 8053555.5000 - val_loss: 1416.4030 - val_mape: 9.0122 - val_mse: 8800612.0000 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1192.1622 - mape: 7.8259 - mse: 8055613.0000\n",
      "Epoch 86: val_loss did not improve from 1251.93970\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1192.1047 - mape: 7.8252 - mse: 8055311.5000 - val_loss: 1276.6646 - val_mape: 8.7191 - val_mse: 8888206.0000 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1187.5985 - mape: 7.7968 - mse: 7995441.0000\n",
      "Epoch 87: val_loss improved from 1251.93970 to 1249.18494, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1187.4371 - mape: 7.7963 - mse: 7991602.5000 - val_loss: 1249.1849 - val_mape: 8.3226 - val_mse: 8583666.0000 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1184.2244 - mape: 7.7631 - mse: 7977740.0000\n",
      "Epoch 88: val_loss did not improve from 1249.18494\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1184.0800 - mape: 7.7646 - mse: 7975949.5000 - val_loss: 1252.9161 - val_mape: 8.3255 - val_mse: 8994943.0000 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1180.3577 - mape: 7.7543 - mse: 7996770.5000\n",
      "Epoch 89: val_loss did not improve from 1249.18494\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1180.0807 - mape: 7.7538 - mse: 7996017.0000 - val_loss: 1250.6367 - val_mape: 8.3548 - val_mse: 8699569.0000 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 1175.5743 - mape: 7.7112 - mse: 7933081.0000\n",
      "Epoch 90: val_loss improved from 1249.18494 to 1245.42114, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1175.4565 - mape: 7.7108 - mse: 7931687.0000 - val_loss: 1245.4211 - val_mape: 8.4317 - val_mse: 8477672.0000 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1176.1747 - mape: 7.7126 - mse: 7929770.5000\n",
      "Epoch 91: val_loss improved from 1245.42114 to 1237.63245, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1175.9930 - mape: 7.7121 - mse: 7924485.5000 - val_loss: 1237.6324 - val_mape: 8.4448 - val_mse: 8551578.0000 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 1170.4531 - mape: 7.6717 - mse: 7899934.0000\n",
      "Epoch 92: val_loss did not improve from 1237.63245\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1170.5287 - mape: 7.6719 - mse: 7898547.0000 - val_loss: 1243.3480 - val_mape: 8.2488 - val_mse: 8679503.0000 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1168.5182 - mape: 7.6528 - mse: 7889922.5000\n",
      "Epoch 93: val_loss did not improve from 1237.63245\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1168.2645 - mape: 7.6524 - mse: 7885128.0000 - val_loss: 1268.9998 - val_mape: 8.3672 - val_mse: 8840048.0000 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1163.9871 - mape: 7.6227 - mse: 7890323.5000\n",
      "Epoch 94: val_loss improved from 1237.63245 to 1227.21704, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1163.7689 - mape: 7.6217 - mse: 7887182.0000 - val_loss: 1227.2170 - val_mape: 8.4664 - val_mse: 8411958.0000 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1162.1926 - mape: 7.6029 - mse: 7858114.0000\n",
      "Epoch 95: val_loss did not improve from 1227.21704\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1162.2148 - mape: 7.6034 - mse: 7857398.0000 - val_loss: 1230.0328 - val_mape: 8.2211 - val_mse: 8601786.0000 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1157.9751 - mape: 7.6035 - mse: 7799715.0000\n",
      "Epoch 96: val_loss did not improve from 1227.21704\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1158.0413 - mape: 7.6040 - mse: 7803821.0000 - val_loss: 1244.0659 - val_mape: 8.6067 - val_mse: 8539461.0000 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1152.2825 - mape: 7.5652 - mse: 7792994.5000\n",
      "Epoch 97: val_loss did not improve from 1227.21704\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1152.2990 - mape: 7.5653 - mse: 7792950.0000 - val_loss: 1244.6893 - val_mape: 8.2129 - val_mse: 8834620.0000 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 1152.1857 - mape: 7.5407 - mse: 7779863.0000\n",
      "Epoch 98: val_loss improved from 1227.21704 to 1226.03137, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1152.4460 - mape: 7.5405 - mse: 7786355.0000 - val_loss: 1226.0314 - val_mape: 8.2411 - val_mse: 8391242.0000 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1145.8113 - mape: 7.5048 - mse: 7770209.0000\n",
      "Epoch 99: val_loss improved from 1226.03137 to 1212.90784, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1145.6910 - mape: 7.5044 - mse: 7768624.0000 - val_loss: 1212.9078 - val_mape: 8.2315 - val_mse: 8455920.0000 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1145.8353 - mape: 7.4936 - mse: 7754274.5000\n",
      "Epoch 100: val_loss did not improve from 1212.90784\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1145.8427 - mape: 7.4936 - mse: 7754293.5000 - val_loss: 1232.6266 - val_mape: 8.2648 - val_mse: 8411501.0000 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1143.0464 - mape: 7.4934 - mse: 7745490.0000\n",
      "Epoch 101: val_loss improved from 1212.90784 to 1204.47998, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1142.8378 - mape: 7.4932 - mse: 7742325.0000 - val_loss: 1204.4800 - val_mape: 8.2717 - val_mse: 8289046.5000 - lr: 0.0010\n",
      "Epoch 102/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1139.3779 - mape: 7.4688 - mse: 7695183.0000\n",
      "Epoch 102: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1139.1315 - mape: 7.4679 - mse: 7692582.5000 - val_loss: 1216.0963 - val_mape: 8.0823 - val_mse: 8686956.0000 - lr: 0.0010\n",
      "Epoch 103/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1139.3403 - mape: 7.4496 - mse: 7712155.0000\n",
      "Epoch 103: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1139.8243 - mape: 7.4514 - mse: 7718908.0000 - val_loss: 1226.4076 - val_mape: 8.1861 - val_mse: 8690468.0000 - lr: 0.0010\n",
      "Epoch 104/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1136.6438 - mape: 7.4281 - mse: 7722462.0000\n",
      "Epoch 104: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1136.8815 - mape: 7.4292 - mse: 7724660.0000 - val_loss: 1210.2432 - val_mape: 8.2275 - val_mse: 8605966.0000 - lr: 0.0010\n",
      "Epoch 105/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1130.5922 - mape: 7.3917 - mse: 7652600.5000\n",
      "Epoch 105: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1130.6055 - mape: 7.3912 - mse: 7653998.5000 - val_loss: 1221.9468 - val_mape: 8.0892 - val_mse: 8648556.0000 - lr: 0.0010\n",
      "Epoch 106/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 1130.3960 - mape: 7.3976 - mse: 7645011.5000\n",
      "Epoch 106: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1130.3595 - mape: 7.3965 - mse: 7648055.5000 - val_loss: 1257.6101 - val_mape: 8.3994 - val_mse: 8943629.0000 - lr: 0.0010\n",
      "Epoch 107/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1127.4110 - mape: 7.3769 - mse: 7628289.0000\n",
      "Epoch 107: val_loss did not improve from 1204.47998\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1127.5142 - mape: 7.3769 - mse: 7630084.5000 - val_loss: 1212.2635 - val_mape: 8.1945 - val_mse: 8284018.0000 - lr: 0.0010\n",
      "Epoch 108/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1124.6735 - mape: 7.3548 - mse: 7622383.5000\n",
      "Epoch 108: val_loss did not improve from 1204.47998\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 1124.2942 - mape: 7.3537 - mse: 7616137.5000 - val_loss: 1233.4141 - val_mape: 8.4854 - val_mse: 8378127.5000 - lr: 0.0010\n",
      "Epoch 109/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 969.8376 - mape: 6.4493 - mse: 7119063.5000\n",
      "Epoch 109: val_loss improved from 1204.47998 to 1071.42188, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 969.8164 - mape: 6.4492 - mse: 7118879.0000 - val_loss: 1071.4219 - val_mape: 7.4485 - val_mse: 7943977.5000 - lr: 1.0000e-04\n",
      "Epoch 110/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 942.0802 - mape: 6.2670 - mse: 7081799.0000\n",
      "Epoch 110: val_loss improved from 1071.42188 to 1056.16907, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 942.0944 - mape: 6.2673 - mse: 7081740.5000 - val_loss: 1056.1691 - val_mape: 7.2927 - val_mse: 7998449.0000 - lr: 1.0000e-04\n",
      "Epoch 111/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 934.5323 - mape: 6.2166 - mse: 7051891.0000\n",
      "Epoch 111: val_loss improved from 1056.16907 to 1053.31299, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 934.8381 - mape: 6.2183 - mse: 7057485.5000 - val_loss: 1053.3130 - val_mape: 7.3384 - val_mse: 8019253.5000 - lr: 1.0000e-04\n",
      "Epoch 112/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 931.3386 - mape: 6.1944 - mse: 7079251.0000\n",
      "Epoch 112: val_loss improved from 1053.31299 to 1049.70093, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 930.9810 - mape: 6.1925 - mse: 7073806.5000 - val_loss: 1049.7009 - val_mape: 7.2733 - val_mse: 7989616.0000 - lr: 1.0000e-04\n",
      "Epoch 113/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 927.6824 - mape: 6.1713 - mse: 7074091.0000\n",
      "Epoch 113: val_loss improved from 1049.70093 to 1049.42053, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 927.6419 - mape: 6.1716 - mse: 7073355.0000 - val_loss: 1049.4205 - val_mape: 7.2698 - val_mse: 7978192.0000 - lr: 1.0000e-04\n",
      "Epoch 114/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 924.8954 - mape: 6.1524 - mse: 7061111.5000\n",
      "Epoch 114: val_loss improved from 1049.42053 to 1047.58459, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 924.8381 - mape: 6.1522 - mse: 7060087.0000 - val_loss: 1047.5846 - val_mape: 7.3367 - val_mse: 7979866.5000 - lr: 1.0000e-04\n",
      "Epoch 115/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 922.5268 - mape: 6.1386 - mse: 7070650.5000\n",
      "Epoch 115: val_loss improved from 1047.58459 to 1047.20093, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 922.4755 - mape: 6.1381 - mse: 7070212.5000 - val_loss: 1047.2009 - val_mape: 7.2596 - val_mse: 8011729.0000 - lr: 1.0000e-04\n",
      "Epoch 116/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 921.0255 - mape: 6.1238 - mse: 7059560.5000\n",
      "Epoch 116: val_loss improved from 1047.20093 to 1044.52991, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 920.8369 - mape: 6.1241 - mse: 7055918.0000 - val_loss: 1044.5299 - val_mape: 7.2318 - val_mse: 7970835.0000 - lr: 1.0000e-04\n",
      "Epoch 117/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 919.2965 - mape: 6.1137 - mse: 7066696.0000\n",
      "Epoch 117: val_loss improved from 1044.52991 to 1043.61938, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 919.2726 - mape: 6.1131 - mse: 7066152.0000 - val_loss: 1043.6194 - val_mape: 7.2704 - val_mse: 8010736.5000 - lr: 1.0000e-04\n",
      "Epoch 118/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 917.9686 - mape: 6.1038 - mse: 7058769.0000\n",
      "Epoch 118: val_loss improved from 1043.61938 to 1040.53589, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 917.7650 - mape: 6.1040 - mse: 7054986.0000 - val_loss: 1040.5359 - val_mape: 7.2357 - val_mse: 8008539.0000 - lr: 1.0000e-04\n",
      "Epoch 119/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 916.1467 - mape: 6.0917 - mse: 7047018.0000\n",
      "Epoch 119: val_loss did not improve from 1040.53589\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 916.1900 - mape: 6.0927 - mse: 7046571.5000 - val_loss: 1041.7469 - val_mape: 7.2269 - val_mse: 8073680.5000 - lr: 1.0000e-04\n",
      "Epoch 120/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 914.9088 - mape: 6.0796 - mse: 7053490.5000\n",
      "Epoch 120: val_loss did not improve from 1040.53589\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 914.7520 - mape: 6.0799 - mse: 7049268.5000 - val_loss: 1041.3444 - val_mape: 7.2429 - val_mse: 7978397.0000 - lr: 1.0000e-04\n",
      "Epoch 121/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 913.5741 - mape: 6.0737 - mse: 7055168.5000\n",
      "Epoch 121: val_loss did not improve from 1040.53589\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 913.5145 - mape: 6.0732 - mse: 7052269.5000 - val_loss: 1041.3708 - val_mape: 7.2758 - val_mse: 8028531.0000 - lr: 1.0000e-04\n",
      "Epoch 122/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 912.5817 - mape: 6.0665 - mse: 7047378.0000\n",
      "Epoch 122: val_loss improved from 1040.53589 to 1038.44373, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 912.6833 - mape: 6.0666 - mse: 7045245.0000 - val_loss: 1038.4437 - val_mape: 7.2072 - val_mse: 7989640.0000 - lr: 1.0000e-04\n",
      "Epoch 123/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 910.4653 - mape: 6.0548 - mse: 7037770.5000\n",
      "Epoch 123: val_loss improved from 1038.44373 to 1036.63965, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 910.7656 - mape: 6.0546 - mse: 7038611.0000 - val_loss: 1036.6396 - val_mape: 7.2113 - val_mse: 8040584.5000 - lr: 1.0000e-04\n",
      "Epoch 124/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 910.3071 - mape: 6.0457 - mse: 7049694.0000\n",
      "Epoch 124: val_loss improved from 1036.63965 to 1034.58704, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 910.0477 - mape: 6.0462 - mse: 7045494.0000 - val_loss: 1034.5870 - val_mape: 7.1756 - val_mse: 8002049.5000 - lr: 1.0000e-04\n",
      "Epoch 125/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 909.3605 - mape: 6.0453 - mse: 7039040.0000\n",
      "Epoch 125: val_loss improved from 1034.58704 to 1034.35913, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 909.2482 - mape: 6.0449 - mse: 7037663.5000 - val_loss: 1034.3591 - val_mape: 7.1933 - val_mse: 8033631.0000 - lr: 1.0000e-04\n",
      "Epoch 126/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 908.4594 - mape: 6.0355 - mse: 7047027.0000\n",
      "Epoch 126: val_loss did not improve from 1034.35913\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 908.0692 - mape: 6.0362 - mse: 7038748.0000 - val_loss: 1036.7620 - val_mape: 7.2295 - val_mse: 8032320.5000 - lr: 1.0000e-04\n",
      "Epoch 127/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 907.2248 - mape: 6.0284 - mse: 7032378.0000\n",
      "Epoch 127: val_loss did not improve from 1034.35913\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 907.2248 - mape: 6.0284 - mse: 7032378.0000 - val_loss: 1035.4434 - val_mape: 7.2067 - val_mse: 8053390.0000 - lr: 1.0000e-04\n",
      "Epoch 128/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 906.2590 - mape: 6.0230 - mse: 7024299.5000\n",
      "Epoch 128: val_loss did not improve from 1034.35913\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 906.3532 - mape: 6.0221 - mse: 7027294.5000 - val_loss: 1035.8960 - val_mape: 7.1872 - val_mse: 8081041.0000 - lr: 1.0000e-04\n",
      "Epoch 129/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 905.5121 - mape: 6.0230 - mse: 7031969.5000\n",
      "Epoch 129: val_loss improved from 1034.35913 to 1033.30823, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 905.5958 - mape: 6.0214 - mse: 7033739.5000 - val_loss: 1033.3082 - val_mape: 7.1768 - val_mse: 7987966.5000 - lr: 1.0000e-04\n",
      "Epoch 130/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 904.6771 - mape: 6.0118 - mse: 7021276.0000\n",
      "Epoch 130: val_loss improved from 1033.30823 to 1032.66455, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 905.0071 - mape: 6.0125 - mse: 7034278.0000 - val_loss: 1032.6646 - val_mape: 7.1959 - val_mse: 7984522.0000 - lr: 1.0000e-04\n",
      "Epoch 131/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 903.8420 - mape: 6.0027 - mse: 7026905.0000\n",
      "Epoch 131: val_loss did not improve from 1032.66455\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 903.7596 - mape: 6.0036 - mse: 7024767.5000 - val_loss: 1033.9510 - val_mape: 7.1702 - val_mse: 8077662.0000 - lr: 1.0000e-04\n",
      "Epoch 132/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 902.9943 - mape: 6.0019 - mse: 7030407.0000\n",
      "Epoch 132: val_loss did not improve from 1032.66455\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 903.1688 - mape: 6.0022 - mse: 7029706.0000 - val_loss: 1036.9728 - val_mape: 7.1901 - val_mse: 8052904.0000 - lr: 1.0000e-04\n",
      "Epoch 133/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 902.0738 - mape: 5.9953 - mse: 7019901.0000\n",
      "Epoch 133: val_loss improved from 1032.66455 to 1031.74084, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 901.9418 - mape: 5.9934 - mse: 7016662.5000 - val_loss: 1031.7408 - val_mape: 7.1773 - val_mse: 8034514.0000 - lr: 1.0000e-04\n",
      "Epoch 134/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 902.3498 - mape: 5.9884 - mse: 7026776.0000\n",
      "Epoch 134: val_loss improved from 1031.74084 to 1029.08130, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 901.5580 - mape: 5.9868 - mse: 7014846.5000 - val_loss: 1029.0813 - val_mape: 7.1915 - val_mse: 8013430.0000 - lr: 1.0000e-04\n",
      "Epoch 135/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 900.4349 - mape: 5.9831 - mse: 7021047.5000\n",
      "Epoch 135: val_loss did not improve from 1029.08130\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 900.5118 - mape: 5.9827 - mse: 7022057.5000 - val_loss: 1037.4795 - val_mape: 7.1982 - val_mse: 8090716.5000 - lr: 1.0000e-04\n",
      "Epoch 136/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 900.2968 - mape: 5.9791 - mse: 7016745.0000\n",
      "Epoch 136: val_loss did not improve from 1029.08130\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 900.1021 - mape: 5.9783 - mse: 7014001.0000 - val_loss: 1029.2787 - val_mape: 7.1863 - val_mse: 8005986.5000 - lr: 1.0000e-04\n",
      "Epoch 137/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 899.3895 - mape: 5.9740 - mse: 7006922.5000\n",
      "Epoch 137: val_loss improved from 1029.08130 to 1028.48779, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 899.6371 - mape: 5.9753 - mse: 7011856.0000 - val_loss: 1028.4878 - val_mape: 7.1765 - val_mse: 8028184.5000 - lr: 1.0000e-04\n",
      "Epoch 138/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 898.5096 - mape: 5.9679 - mse: 7006686.5000\n",
      "Epoch 138: val_loss did not improve from 1028.48779\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 898.3397 - mape: 5.9674 - mse: 7007177.5000 - val_loss: 1029.3513 - val_mape: 7.1475 - val_mse: 7970852.5000 - lr: 1.0000e-04\n",
      "Epoch 139/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 897.3071 - mape: 5.9633 - mse: 7005085.5000\n",
      "Epoch 139: val_loss did not improve from 1028.48779\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 897.5048 - mape: 5.9637 - mse: 7006788.5000 - val_loss: 1031.5060 - val_mape: 7.1705 - val_mse: 8037377.5000 - lr: 1.0000e-04\n",
      "Epoch 140/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 897.0404 - mape: 5.9561 - mse: 7003435.0000\n",
      "Epoch 140: val_loss improved from 1028.48779 to 1027.04712, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 897.0581 - mape: 5.9561 - mse: 7003555.5000 - val_loss: 1027.0471 - val_mape: 7.1565 - val_mse: 8010751.0000 - lr: 1.0000e-04\n",
      "Epoch 141/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 895.9676 - mape: 5.9519 - mse: 6994981.0000\n",
      "Epoch 141: val_loss did not improve from 1027.04712\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 896.1327 - mape: 5.9514 - mse: 6998132.5000 - val_loss: 1038.3923 - val_mape: 7.2222 - val_mse: 7910455.0000 - lr: 1.0000e-04\n",
      "Epoch 142/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 896.4147 - mape: 5.9481 - mse: 7016328.5000\n",
      "Epoch 142: val_loss improved from 1027.04712 to 1026.38318, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 895.8134 - mape: 5.9481 - mse: 7005413.5000 - val_loss: 1026.3832 - val_mape: 7.1665 - val_mse: 7997490.0000 - lr: 1.0000e-04\n",
      "Epoch 143/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 895.2062 - mape: 5.9454 - mse: 6997257.0000\n",
      "Epoch 143: val_loss did not improve from 1026.38318\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 895.0817 - mape: 5.9448 - mse: 6995182.5000 - val_loss: 1035.5869 - val_mape: 7.1778 - val_mse: 8117648.0000 - lr: 1.0000e-04\n",
      "Epoch 144/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 894.4487 - mape: 5.9411 - mse: 6993943.0000\n",
      "Epoch 144: val_loss did not improve from 1026.38318\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 894.4487 - mape: 5.9411 - mse: 6993943.0000 - val_loss: 1034.0538 - val_mape: 7.2314 - val_mse: 7939036.0000 - lr: 1.0000e-04\n",
      "Epoch 145/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 893.8770 - mape: 5.9348 - mse: 6988035.0000\n",
      "Epoch 145: val_loss did not improve from 1026.38318\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 894.0339 - mape: 5.9353 - mse: 6991092.5000 - val_loss: 1028.5405 - val_mape: 7.1790 - val_mse: 7992753.0000 - lr: 1.0000e-04\n",
      "Epoch 146/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 893.0013 - mape: 5.9297 - mse: 6987361.5000\n",
      "Epoch 146: val_loss did not improve from 1026.38318\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 892.9749 - mape: 5.9276 - mse: 6987799.0000 - val_loss: 1028.6451 - val_mape: 7.1932 - val_mse: 7979084.5000 - lr: 1.0000e-04\n",
      "Epoch 147/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 892.6545 - mape: 5.9245 - mse: 6993067.0000\n",
      "Epoch 147: val_loss improved from 1026.38318 to 1025.79211, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 892.5000 - mape: 5.9243 - mse: 6990609.0000 - val_loss: 1025.7921 - val_mape: 7.1372 - val_mse: 7987714.0000 - lr: 1.0000e-04\n",
      "Epoch 148/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 892.4247 - mape: 5.9220 - mse: 6989309.0000\n",
      "Epoch 148: val_loss improved from 1025.79211 to 1025.43311, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 892.1880 - mape: 5.9218 - mse: 6986022.0000 - val_loss: 1025.4331 - val_mape: 7.1458 - val_mse: 8032497.5000 - lr: 1.0000e-04\n",
      "Epoch 149/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 891.9260 - mape: 5.9198 - mse: 6984240.5000\n",
      "Epoch 149: val_loss did not improve from 1025.43311\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 891.5941 - mape: 5.9199 - mse: 6978788.0000 - val_loss: 1025.7561 - val_mape: 7.1357 - val_mse: 8042558.0000 - lr: 1.0000e-04\n",
      "Epoch 150/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 890.8408 - mape: 5.9132 - mse: 6978773.0000\n",
      "Epoch 150: val_loss did not improve from 1025.43311\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 890.8254 - mape: 5.9131 - mse: 6978598.0000 - val_loss: 1025.4717 - val_mape: 7.1400 - val_mse: 8043011.5000 - lr: 1.0000e-04\n",
      "Epoch 151/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 890.1421 - mape: 5.9100 - mse: 6985814.0000\n",
      "Epoch 151: val_loss did not improve from 1025.43311\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 890.1421 - mape: 5.9100 - mse: 6985814.0000 - val_loss: 1026.2970 - val_mape: 7.1179 - val_mse: 8022070.0000 - lr: 1.0000e-04\n",
      "Epoch 152/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 890.0685 - mape: 5.9061 - mse: 6990816.5000\n",
      "Epoch 152: val_loss improved from 1025.43311 to 1024.35681, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 889.6046 - mape: 5.9053 - mse: 6983163.5000 - val_loss: 1024.3568 - val_mape: 7.1194 - val_mse: 8032376.5000 - lr: 1.0000e-04\n",
      "Epoch 153/500\n",
      "5988/6006 [============================>.] - ETA: 0s - loss: 889.3414 - mape: 5.9006 - mse: 6979481.5000\n",
      "Epoch 153: val_loss did not improve from 1024.35681\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 889.2112 - mape: 5.9009 - mse: 6975109.5000 - val_loss: 1026.3118 - val_mape: 7.1582 - val_mse: 7953489.0000 - lr: 1.0000e-04\n",
      "Epoch 154/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 888.8191 - mape: 5.9000 - mse: 6976446.5000\n",
      "Epoch 154: val_loss improved from 1024.35681 to 1021.93304, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 888.7137 - mape: 5.8996 - mse: 6975357.5000 - val_loss: 1021.9330 - val_mape: 7.1217 - val_mse: 8033101.5000 - lr: 1.0000e-04\n",
      "Epoch 155/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 888.1728 - mape: 5.8921 - mse: 6969006.5000\n",
      "Epoch 155: val_loss did not improve from 1021.93304\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 888.1580 - mape: 5.8920 - mse: 6968829.5000 - val_loss: 1027.8748 - val_mape: 7.1572 - val_mse: 7960772.0000 - lr: 1.0000e-04\n",
      "Epoch 156/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 887.1087 - mape: 5.8850 - mse: 6976087.5000\n",
      "Epoch 156: val_loss did not improve from 1021.93304\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 887.1981 - mape: 5.8852 - mse: 6976115.0000 - val_loss: 1024.2629 - val_mape: 7.1431 - val_mse: 8021532.5000 - lr: 1.0000e-04\n",
      "Epoch 157/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 887.2227 - mape: 5.8830 - mse: 6973032.0000\n",
      "Epoch 157: val_loss improved from 1021.93304 to 1020.62506, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 887.0561 - mape: 5.8835 - mse: 6966982.5000 - val_loss: 1020.6251 - val_mape: 7.1091 - val_mse: 8015550.0000 - lr: 1.0000e-04\n",
      "Epoch 158/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 886.3618 - mape: 5.8802 - mse: 6968458.0000\n",
      "Epoch 158: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 886.3198 - mape: 5.8803 - mse: 6969467.0000 - val_loss: 1026.6101 - val_mape: 7.1480 - val_mse: 7946742.5000 - lr: 1.0000e-04\n",
      "Epoch 159/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 885.7414 - mape: 5.8761 - mse: 6959602.5000\n",
      "Epoch 159: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 885.9169 - mape: 5.8765 - mse: 6961590.5000 - val_loss: 1026.0594 - val_mape: 7.1043 - val_mse: 8077149.0000 - lr: 1.0000e-04\n",
      "Epoch 160/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 885.3168 - mape: 5.8723 - mse: 6964273.0000\n",
      "Epoch 160: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 885.3168 - mape: 5.8723 - mse: 6964273.0000 - val_loss: 1022.8060 - val_mape: 7.1469 - val_mse: 8022113.5000 - lr: 1.0000e-04\n",
      "Epoch 161/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 884.8448 - mape: 5.8685 - mse: 6960098.5000\n",
      "Epoch 161: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 884.8884 - mape: 5.8700 - mse: 6962881.5000 - val_loss: 1021.2150 - val_mape: 7.1206 - val_mse: 7998688.5000 - lr: 1.0000e-04\n",
      "Epoch 162/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 884.7576 - mape: 5.8668 - mse: 6964305.0000\n",
      "Epoch 162: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 884.3729 - mape: 5.8652 - mse: 6957063.5000 - val_loss: 1024.4211 - val_mape: 7.1319 - val_mse: 8043540.0000 - lr: 1.0000e-04\n",
      "Epoch 163/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 883.6091 - mape: 5.8593 - mse: 6961225.5000\n",
      "Epoch 163: val_loss did not improve from 1020.62506\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 883.6091 - mape: 5.8593 - mse: 6961225.5000 - val_loss: 1023.0689 - val_mape: 7.1397 - val_mse: 8036767.5000 - lr: 1.0000e-04\n",
      "Epoch 164/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 883.2460 - mape: 5.8581 - mse: 6949121.0000\n",
      "Epoch 164: val_loss did not improve from 1020.62506\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 883.1610 - mape: 5.8579 - mse: 6947825.0000 - val_loss: 1021.9009 - val_mape: 7.1670 - val_mse: 8013354.5000 - lr: 1.0000e-04\n",
      "Epoch 165/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 867.0969 - mape: 5.7649 - mse: 6913330.0000\n",
      "Epoch 165: val_loss improved from 1020.62506 to 1011.35083, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 867.0590 - mape: 5.7642 - mse: 6914366.0000 - val_loss: 1011.3508 - val_mape: 7.0664 - val_mse: 8002262.0000 - lr: 1.0000e-05\n",
      "Epoch 166/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 864.9113 - mape: 5.7504 - mse: 6922086.0000\n",
      "Epoch 166: val_loss improved from 1011.35083 to 1011.11829, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 864.5874 - mape: 5.7482 - mse: 6916997.0000 - val_loss: 1011.1183 - val_mape: 7.0787 - val_mse: 7995040.0000 - lr: 1.0000e-05\n",
      "Epoch 167/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 863.8367 - mape: 5.7438 - mse: 6913412.0000\n",
      "Epoch 167: val_loss improved from 1011.11829 to 1010.68781, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 863.9902 - mape: 5.7441 - mse: 6915845.0000 - val_loss: 1010.6878 - val_mape: 7.0648 - val_mse: 8000411.5000 - lr: 1.0000e-05\n",
      "Epoch 168/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 863.5866 - mape: 5.7413 - mse: 6915108.5000\n",
      "Epoch 168: val_loss did not improve from 1010.68781\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 863.6202 - mape: 5.7421 - mse: 6916610.0000 - val_loss: 1010.7598 - val_mape: 7.0615 - val_mse: 8008595.5000 - lr: 1.0000e-05\n",
      "Epoch 169/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 863.3322 - mape: 5.7402 - mse: 6917932.0000\n",
      "Epoch 169: val_loss improved from 1010.68781 to 1010.42114, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 863.3322 - mape: 5.7402 - mse: 6917932.0000 - val_loss: 1010.4211 - val_mape: 7.0603 - val_mse: 7991994.5000 - lr: 1.0000e-05\n",
      "Epoch 170/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 863.1761 - mape: 5.7392 - mse: 6914901.0000\n",
      "Epoch 170: val_loss improved from 1010.42114 to 1010.04156, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 863.1652 - mape: 5.7391 - mse: 6915422.5000 - val_loss: 1010.0416 - val_mape: 7.0645 - val_mse: 8002877.5000 - lr: 1.0000e-05\n",
      "Epoch 171/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 863.2797 - mape: 5.7404 - mse: 6920877.0000\n",
      "Epoch 171: val_loss improved from 1010.04156 to 1009.92645, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.9719 - mape: 5.7386 - mse: 6918453.5000 - val_loss: 1009.9265 - val_mape: 7.0570 - val_mse: 8006441.5000 - lr: 1.0000e-05\n",
      "Epoch 172/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 862.5647 - mape: 5.7374 - mse: 6913788.5000\n",
      "Epoch 172: val_loss improved from 1009.92645 to 1009.78668, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.8318 - mape: 5.7369 - mse: 6918660.5000 - val_loss: 1009.7867 - val_mape: 7.0638 - val_mse: 8003068.5000 - lr: 1.0000e-05\n",
      "Epoch 173/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 863.1992 - mape: 5.7364 - mse: 6928463.0000\n",
      "Epoch 173: val_loss did not improve from 1009.78668\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.6724 - mape: 5.7349 - mse: 6919490.0000 - val_loss: 1010.1091 - val_mape: 7.0800 - val_mse: 7986365.0000 - lr: 1.0000e-05\n",
      "Epoch 174/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 862.6094 - mape: 5.7363 - mse: 6916685.5000\n",
      "Epoch 174: val_loss did not improve from 1009.78668\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.4906 - mape: 5.7359 - mse: 6915396.0000 - val_loss: 1009.8166 - val_mape: 7.0685 - val_mse: 8004049.0000 - lr: 1.0000e-05\n",
      "Epoch 175/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 862.4974 - mape: 5.7333 - mse: 6919445.5000\n",
      "Epoch 175: val_loss improved from 1009.78668 to 1009.76135, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.3717 - mape: 5.7339 - mse: 6916875.5000 - val_loss: 1009.7614 - val_mape: 7.0679 - val_mse: 7997188.5000 - lr: 1.0000e-05\n",
      "Epoch 176/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 862.5264 - mape: 5.7345 - mse: 6924101.5000\n",
      "Epoch 176: val_loss improved from 1009.76135 to 1009.63354, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.3677 - mape: 5.7339 - mse: 6920095.5000 - val_loss: 1009.6335 - val_mape: 7.0606 - val_mse: 7988977.0000 - lr: 1.0000e-05\n",
      "Epoch 177/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 862.2969 - mape: 5.7340 - mse: 6916700.5000\n",
      "Epoch 177: val_loss did not improve from 1009.63354\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.2068 - mape: 5.7329 - mse: 6918263.5000 - val_loss: 1009.6587 - val_mape: 7.0712 - val_mse: 7989503.0000 - lr: 1.0000e-05\n",
      "Epoch 178/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 862.1105 - mape: 5.7317 - mse: 6919574.5000\n",
      "Epoch 178: val_loss improved from 1009.63354 to 1009.49451, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 862.0963 - mape: 5.7317 - mse: 6919400.0000 - val_loss: 1009.4945 - val_mape: 7.0579 - val_mse: 7999525.5000 - lr: 1.0000e-05\n",
      "Epoch 179/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 862.0267 - mape: 5.7317 - mse: 6919075.5000\n",
      "Epoch 179: val_loss did not improve from 1009.49451\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.9828 - mape: 5.7319 - mse: 6918840.0000 - val_loss: 1009.5286 - val_mape: 7.0626 - val_mse: 7995162.0000 - lr: 1.0000e-05\n",
      "Epoch 180/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 861.9753 - mape: 5.7310 - mse: 6922096.0000\n",
      "Epoch 180: val_loss did not improve from 1009.49451\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.8516 - mape: 5.7299 - mse: 6919934.0000 - val_loss: 1011.1535 - val_mape: 7.0705 - val_mse: 7973086.5000 - lr: 1.0000e-05\n",
      "Epoch 181/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 861.8741 - mape: 5.7309 - mse: 6917339.0000\n",
      "Epoch 181: val_loss improved from 1009.49451 to 1009.28979, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.8741 - mape: 5.7309 - mse: 6917339.0000 - val_loss: 1009.2898 - val_mape: 7.0692 - val_mse: 7988289.5000 - lr: 1.0000e-05\n",
      "Epoch 182/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 861.7111 - mape: 5.7280 - mse: 6919752.0000\n",
      "Epoch 182: val_loss improved from 1009.28979 to 1009.21027, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.6633 - mape: 5.7287 - mse: 6918869.0000 - val_loss: 1009.2103 - val_mape: 7.0717 - val_mse: 7999819.5000 - lr: 1.0000e-05\n",
      "Epoch 183/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 861.3665 - mape: 5.7284 - mse: 6917417.0000\n",
      "Epoch 183: val_loss did not improve from 1009.21027\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.5743 - mape: 5.7291 - mse: 6918400.5000 - val_loss: 1009.2139 - val_mape: 7.0657 - val_mse: 7992370.5000 - lr: 1.0000e-05\n",
      "Epoch 184/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 861.6412 - mape: 5.7281 - mse: 6920574.0000\n",
      "Epoch 184: val_loss did not improve from 1009.21027\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.5510 - mape: 5.7284 - mse: 6918895.5000 - val_loss: 1009.5358 - val_mape: 7.0636 - val_mse: 8013658.0000 - lr: 1.0000e-05\n",
      "Epoch 185/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 861.1793 - mape: 5.7277 - mse: 6907999.5000\n",
      "Epoch 185: val_loss did not improve from 1009.21027\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.5088 - mape: 5.7272 - mse: 6917398.5000 - val_loss: 1009.4902 - val_mape: 7.0713 - val_mse: 7992134.5000 - lr: 1.0000e-05\n",
      "Epoch 186/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 861.4376 - mape: 5.7271 - mse: 6923474.5000\n",
      "Epoch 186: val_loss did not improve from 1009.21027\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.3517 - mape: 5.7265 - mse: 6920690.5000 - val_loss: 1009.3057 - val_mape: 7.0709 - val_mse: 7990128.5000 - lr: 1.0000e-05\n",
      "Epoch 187/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 861.0744 - mape: 5.7270 - mse: 6909394.5000\n",
      "Epoch 187: val_loss did not improve from 1009.21027\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.3061 - mape: 5.7270 - mse: 6918102.5000 - val_loss: 1009.2894 - val_mape: 7.0495 - val_mse: 7991227.5000 - lr: 1.0000e-05\n",
      "Epoch 188/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 861.0079 - mape: 5.7257 - mse: 6911935.0000\n",
      "Epoch 188: val_loss improved from 1009.21027 to 1009.04926, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.1692 - mape: 5.7253 - mse: 6918076.5000 - val_loss: 1009.0493 - val_mape: 7.0555 - val_mse: 7999014.5000 - lr: 1.0000e-05\n",
      "Epoch 189/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 861.4157 - mape: 5.7253 - mse: 6923814.0000\n",
      "Epoch 189: val_loss improved from 1009.04926 to 1009.04840, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.1143 - mape: 5.7251 - mse: 6919389.5000 - val_loss: 1009.0484 - val_mape: 7.0697 - val_mse: 7997379.5000 - lr: 1.0000e-05\n",
      "Epoch 190/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 860.9731 - mape: 5.7243 - mse: 6917793.0000\n",
      "Epoch 190: val_loss improved from 1009.04840 to 1008.93359, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 861.0187 - mape: 5.7254 - mse: 6916720.0000 - val_loss: 1008.9336 - val_mape: 7.0461 - val_mse: 7991380.5000 - lr: 1.0000e-05\n",
      "Epoch 191/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 861.2119 - mape: 5.7233 - mse: 6919332.5000\n",
      "Epoch 191: val_loss did not improve from 1008.93359\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.9576 - mape: 5.7238 - mse: 6917472.0000 - val_loss: 1009.0712 - val_mape: 7.0667 - val_mse: 7989441.5000 - lr: 1.0000e-05\n",
      "Epoch 192/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 861.1857 - mape: 5.7249 - mse: 6923975.5000\n",
      "Epoch 192: val_loss improved from 1008.93359 to 1008.71588, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.8720 - mape: 5.7240 - mse: 6919140.5000 - val_loss: 1008.7159 - val_mape: 7.0619 - val_mse: 8004050.0000 - lr: 1.0000e-05\n",
      "Epoch 193/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 860.9241 - mape: 5.7227 - mse: 6920148.5000\n",
      "Epoch 193: val_loss did not improve from 1008.71588\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.7170 - mape: 5.7230 - mse: 6918219.0000 - val_loss: 1008.7686 - val_mape: 7.0617 - val_mse: 7995274.5000 - lr: 1.0000e-05\n",
      "Epoch 194/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 860.4871 - mape: 5.7223 - mse: 6911274.5000\n",
      "Epoch 194: val_loss did not improve from 1008.71588\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.7269 - mape: 5.7230 - mse: 6917261.0000 - val_loss: 1009.0353 - val_mape: 7.0633 - val_mse: 7997484.0000 - lr: 1.0000e-05\n",
      "Epoch 195/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 860.6556 - mape: 5.7222 - mse: 6920077.5000\n",
      "Epoch 195: val_loss did not improve from 1008.71588\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.6347 - mape: 5.7222 - mse: 6918736.0000 - val_loss: 1009.0312 - val_mape: 7.0647 - val_mse: 7993149.5000 - lr: 1.0000e-05\n",
      "Epoch 196/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 860.6618 - mape: 5.7226 - mse: 6919306.0000\n",
      "Epoch 196: val_loss improved from 1008.71588 to 1008.69354, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.6027 - mape: 5.7223 - mse: 6919222.5000 - val_loss: 1008.6935 - val_mape: 7.0570 - val_mse: 8005831.0000 - lr: 1.0000e-05\n",
      "Epoch 197/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 860.1920 - mape: 5.7210 - mse: 6907524.5000\n",
      "Epoch 197: val_loss did not improve from 1008.69354\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.5651 - mape: 5.7219 - mse: 6918738.0000 - val_loss: 1008.9270 - val_mape: 7.0677 - val_mse: 8007686.5000 - lr: 1.0000e-05\n",
      "Epoch 198/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 860.5920 - mape: 5.7202 - mse: 6922752.5000\n",
      "Epoch 198: val_loss improved from 1008.69354 to 1008.36194, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.4271 - mape: 5.7206 - mse: 6919727.0000 - val_loss: 1008.3619 - val_mape: 7.0544 - val_mse: 7997746.0000 - lr: 1.0000e-05\n",
      "Epoch 199/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 860.3714 - mape: 5.7202 - mse: 6914963.0000\n",
      "Epoch 199: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.3615 - mape: 5.7201 - mse: 6915892.5000 - val_loss: 1008.6386 - val_mape: 7.0622 - val_mse: 8003174.5000 - lr: 1.0000e-05\n",
      "Epoch 200/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 860.1719 - mape: 5.7195 - mse: 6918093.5000\n",
      "Epoch 200: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.2670 - mape: 5.7194 - mse: 6918252.0000 - val_loss: 1008.7348 - val_mape: 7.0568 - val_mse: 7998505.5000 - lr: 1.0000e-05\n",
      "Epoch 201/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 860.0737 - mape: 5.7194 - mse: 6915126.5000\n",
      "Epoch 201: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.1461 - mape: 5.7192 - mse: 6917393.0000 - val_loss: 1008.5828 - val_mape: 7.0717 - val_mse: 7996494.0000 - lr: 1.0000e-05\n",
      "Epoch 202/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 860.1741 - mape: 5.7180 - mse: 6921827.0000\n",
      "Epoch 202: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 860.0728 - mape: 5.7184 - mse: 6919457.0000 - val_loss: 1008.5058 - val_mape: 7.0623 - val_mse: 7989080.0000 - lr: 1.0000e-05\n",
      "Epoch 203/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 859.9146 - mape: 5.7183 - mse: 6917701.0000\n",
      "Epoch 203: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 859.9917 - mape: 5.7188 - mse: 6916888.5000 - val_loss: 1008.5460 - val_mape: 7.0620 - val_mse: 7999056.0000 - lr: 1.0000e-05\n",
      "Epoch 204/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 859.8695 - mape: 5.7173 - mse: 6914775.5000\n",
      "Epoch 204: val_loss did not improve from 1008.36194\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 859.9466 - mape: 5.7175 - mse: 6918274.5000 - val_loss: 1008.9683 - val_mape: 7.0623 - val_mse: 7980515.5000 - lr: 1.0000e-05\n",
      "Epoch 205/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 859.7161 - mape: 5.7171 - mse: 6909192.5000\n",
      "Epoch 205: val_loss did not improve from 1008.36194\n",
      "\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 859.9003 - mape: 5.7168 - mse: 6916297.0000 - val_loss: 1009.4988 - val_mape: 7.0748 - val_mse: 7976161.5000 - lr: 1.0000e-05\n",
      "Epoch 206/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 858.4615 - mape: 5.7079 - mse: 6917477.5000\n",
      "Epoch 206: val_loss improved from 1008.36194 to 1007.88611, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 858.1713 - mape: 5.7080 - mse: 6912292.5000 - val_loss: 1007.8861 - val_mape: 7.0591 - val_mse: 7997429.5000 - lr: 1.0000e-06\n",
      "Epoch 207/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 858.0118 - mape: 5.7054 - mse: 6916978.0000\n",
      "Epoch 207: val_loss improved from 1007.88611 to 1007.83649, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.8844 - mape: 5.7052 - mse: 6913699.0000 - val_loss: 1007.8365 - val_mape: 7.0561 - val_mse: 7997306.5000 - lr: 1.0000e-06\n",
      "Epoch 208/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 857.5880 - mape: 5.7028 - mse: 6914256.5000\n",
      "Epoch 208: val_loss did not improve from 1007.83649\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.8171 - mape: 5.7045 - mse: 6912833.5000 - val_loss: 1007.8642 - val_mape: 7.0578 - val_mse: 7996907.5000 - lr: 1.0000e-06\n",
      "Epoch 209/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 857.8050 - mape: 5.7044 - mse: 6913466.0000\n",
      "Epoch 209: val_loss did not improve from 1007.83649\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7917 - mape: 5.7043 - mse: 6913291.5000 - val_loss: 1007.8368 - val_mape: 7.0583 - val_mse: 7999545.0000 - lr: 1.0000e-06\n",
      "Epoch 210/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 858.1310 - mape: 5.7053 - mse: 6918887.0000\n",
      "Epoch 210: val_loss did not improve from 1007.83649\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7650 - mape: 5.7043 - mse: 6913137.5000 - val_loss: 1007.8535 - val_mape: 7.0586 - val_mse: 7995208.0000 - lr: 1.0000e-06\n",
      "Epoch 211/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 858.1836 - mape: 5.7049 - mse: 6920591.0000\n",
      "Epoch 211: val_loss did not improve from 1007.83649\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7695 - mape: 5.7043 - mse: 6913358.0000 - val_loss: 1007.9265 - val_mape: 7.0575 - val_mse: 7995313.5000 - lr: 1.0000e-06\n",
      "Epoch 212/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 857.7979 - mape: 5.7041 - mse: 6914131.0000\n",
      "Epoch 212: val_loss did not improve from 1007.83649\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7398 - mape: 5.7040 - mse: 6913174.5000 - val_loss: 1007.8463 - val_mape: 7.0557 - val_mse: 7997566.0000 - lr: 1.0000e-06\n",
      "Epoch 213/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 858.0223 - mape: 5.7054 - mse: 6916817.0000\n",
      "Epoch 213: val_loss improved from 1007.83649 to 1007.81921, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7200 - mape: 5.7039 - mse: 6912800.0000 - val_loss: 1007.8192 - val_mape: 7.0572 - val_mse: 7998236.0000 - lr: 1.0000e-06\n",
      "Epoch 214/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 857.5419 - mape: 5.7033 - mse: 6912792.5000\n",
      "Epoch 214: val_loss improved from 1007.81921 to 1007.79218, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.7056 - mape: 5.7039 - mse: 6912667.0000 - val_loss: 1007.7922 - val_mape: 7.0563 - val_mse: 7998805.0000 - lr: 1.0000e-06\n",
      "Epoch 215/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 857.4622 - mape: 5.7035 - mse: 6909879.0000\n",
      "Epoch 215: val_loss did not improve from 1007.79218\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6913 - mape: 5.7039 - mse: 6912617.0000 - val_loss: 1007.8175 - val_mape: 7.0544 - val_mse: 7998793.5000 - lr: 1.0000e-06\n",
      "Epoch 216/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 857.6257 - mape: 5.7032 - mse: 6913212.5000\n",
      "Epoch 216: val_loss did not improve from 1007.79218\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6754 - mape: 5.7036 - mse: 6913835.5000 - val_loss: 1007.8546 - val_mape: 7.0546 - val_mse: 7999855.0000 - lr: 1.0000e-06\n",
      "Epoch 217/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 857.6851 - mape: 5.7031 - mse: 6913918.5000\n",
      "Epoch 217: val_loss did not improve from 1007.79218\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6575 - mape: 5.7035 - mse: 6912938.5000 - val_loss: 1007.9003 - val_mape: 7.0581 - val_mse: 7994090.5000 - lr: 1.0000e-06\n",
      "Epoch 218/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 857.4656 - mape: 5.7035 - mse: 6909526.0000\n",
      "Epoch 218: val_loss improved from 1007.79218 to 1007.78143, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6484 - mape: 5.7038 - mse: 6912328.5000 - val_loss: 1007.7814 - val_mape: 7.0550 - val_mse: 8000809.5000 - lr: 1.0000e-06\n",
      "Epoch 219/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 857.7502 - mape: 5.7049 - mse: 6914692.0000\n",
      "Epoch 219: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6240 - mape: 5.7032 - mse: 6912757.5000 - val_loss: 1007.8327 - val_mape: 7.0560 - val_mse: 7997338.0000 - lr: 1.0000e-06\n",
      "Epoch 220/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 857.6995 - mape: 5.7036 - mse: 6915203.0000\n",
      "Epoch 220: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6166 - mape: 5.7033 - mse: 6912626.5000 - val_loss: 1007.8632 - val_mape: 7.0596 - val_mse: 7996402.0000 - lr: 1.0000e-06\n",
      "Epoch 221/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 857.5670 - mape: 5.7054 - mse: 6911492.0000\n",
      "Epoch 221: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6187 - mape: 5.7037 - mse: 6912876.5000 - val_loss: 1007.8115 - val_mape: 7.0560 - val_mse: 8000274.5000 - lr: 1.0000e-06\n",
      "Epoch 222/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 857.9510 - mape: 5.7029 - mse: 6918231.0000\n",
      "Epoch 222: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.6053 - mape: 5.7036 - mse: 6912616.5000 - val_loss: 1007.8317 - val_mape: 7.0543 - val_mse: 7996858.0000 - lr: 1.0000e-06\n",
      "Epoch 223/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 857.2729 - mape: 5.7028 - mse: 6909396.5000\n",
      "Epoch 223: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.5949 - mape: 5.7032 - mse: 6912480.0000 - val_loss: 1007.8079 - val_mape: 7.0570 - val_mse: 8000567.5000 - lr: 1.0000e-06\n",
      "Epoch 224/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 857.9238 - mape: 5.7029 - mse: 6918990.0000\n",
      "Epoch 224: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.5786 - mape: 5.7031 - mse: 6913057.0000 - val_loss: 1007.8379 - val_mape: 7.0581 - val_mse: 7997410.0000 - lr: 1.0000e-06\n",
      "Epoch 225/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 857.5812 - mape: 5.7038 - mse: 6912834.5000\n",
      "Epoch 225: val_loss did not improve from 1007.78143\n",
      "\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.5643 - mape: 5.7031 - mse: 6912830.0000 - val_loss: 1007.8784 - val_mape: 7.0580 - val_mse: 7995189.5000 - lr: 1.0000e-06\n",
      "Epoch 226/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 856.9139 - mape: 5.7020 - mse: 6904743.0000\n",
      "Epoch 226: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.3610 - mape: 5.7020 - mse: 6910509.5000 - val_loss: 1007.8384 - val_mape: 7.0570 - val_mse: 7997421.0000 - lr: 1.0000e-07\n",
      "Epoch 227/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 857.0236 - mape: 5.7011 - mse: 6908155.5000\n",
      "Epoch 227: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.3381 - mape: 5.7018 - mse: 6911172.5000 - val_loss: 1007.8338 - val_mape: 7.0572 - val_mse: 7997106.0000 - lr: 1.0000e-07\n",
      "Epoch 228/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 857.1843 - mape: 5.7013 - mse: 6908200.0000\n",
      "Epoch 228: val_loss did not improve from 1007.78143\n",
      "6006/6006 [==============================] - 19s 3ms/step - loss: 857.3290 - mape: 5.7017 - mse: 6911410.5000 - val_loss: 1007.8345 - val_mape: 7.0569 - val_mse: 7997512.0000 - lr: 1.0000e-07\n",
      "Epoch 228: early stopping\n",
      "CPU times: user 1h 51min 50s, sys: 22min 24s, total: 2h 14min 14s\n",
      "Wall time: 1h 11min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training previous model with the call bascks for 500 epochs\n",
    "history_2 = NN_model_2.fit(X_train_ss, y_train, epochs=500, validation_split=0.2, verbose=1, callbacks=[early_stop, model_checkpoint, reduce_lr_loss], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "-bQ3ejrRAdrH",
    "outputId": "29d11118-cd12-44e6-ecc1-8196987f3665"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(history_2.history['loss'], label='train')\n",
    "plt.plot(history_2.history['val_loss'], label='Validation')\n",
    "plt.title('Mean Absolute Error for Each Training Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Observations__\n",
    "- Model training terminated at 228 epochs with a training time of 1 hour and 11 minutes using premium GPUs.\n",
    "- Learning rate was reduced twice and no improvement recorded after a learning rate of $1 x {10^-}{^6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKchQes10Des",
    "outputId": "4705d4e6-e25f-4336-8cdd-b134583cc706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7507/7507 [==============================] - 10s 1ms/step\n",
      "1877/1877 [==============================] - 3s 1ms/step\n",
      "Model Performance\n",
      "-----------------\n",
      "Train set R2 score: 0.9861\n",
      "Test R2 score: 0.9839\n",
      "Test set Adjusted R2 score: 0.9839\n",
      "Test set Root Mean Squared Error - RMSE: 2873.8085\n",
      "Test set Mean Absolute Error - MAE: 1024.4848\n",
      "Test set Mean Absolute Precentage Error - MAPE: 0.0713\n",
      "CPU times: user 21 s, sys: 2.19 s, total: 23.2 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate the model\n",
    "NN_model_2_performance = evaluate_NN(NN_model_2, X_test_ss, y_test, X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is performing extremely well with high R2 scores and low errors scores compared to other models and managed to achieve a MAPE of 7.13%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmucU_Zc5LI8",
    "outputId": "c9069c78-f389-4946-deb4-cf9ed131d133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN_model_2.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "joblib.dump(NN_model_2, 'NN_model_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-EfvUG0rD06s"
   },
   "outputs": [],
   "source": [
    "Best_NN_model = joblib.load('NN_model_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUVShZ76EL9y",
    "outputId": "334465eb-dcae-4933-d609-7730cc498f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7507/7507 [==============================] - 11s 1ms/step\n",
      "1877/1877 [==============================] - 3s 1ms/step\n",
      "Model Performance\n",
      "-----------------\n",
      "Train set R2 score: 0.9861\n",
      "Test R2 score: 0.9839\n",
      "Test set Adjusted R2 score: 0.9839\n",
      "Test set Root Mean Squared Error - RMSE: 2873.8085\n",
      "Test set Mean Absolute Error - MAE: 1024.4848\n",
      "Test set Mean Absolute Precentage Error - MAPE: 0.0713\n",
      "CPU times: user 20.3 s, sys: 1.98 s, total: 22.2 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Best_NN_performance = evaluate_NN(Best_NN_model, X_test_ss, y_test, X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add more layers to see if we can surpass the best lowest MAPE achieved (6.33%) during Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "lU6Yur7UAdtk"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(123) # for consistent results\n",
    "\n",
    "# Create a new sequential model\n",
    "NN_model_3 = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "NN_model_3.add(layers.Dense(512, activation='relu', input_shape= (22,),  name='layer1'))\n",
    "\n",
    "NN_model_3.add(layers.Dense(256, activation='relu',  name='layer2'))\n",
    "\n",
    "NN_model_3.add(layers.Dense(256, activation='relu',  name='layer3'))\n",
    "\n",
    "NN_model_3.add(layers.Dense(128, activation='relu', name='layer4'))\n",
    "\n",
    "NN_model_3.add(layers.Dense(64, activation='relu', name='layer5'))\n",
    "\n",
    "\n",
    "# Declare the output layer\n",
    "NN_model_3.add(layers.Dense(1, activation='relu', name='output')) # ReLU since price cannot have negative values\n",
    "\n",
    "# Compile the model\n",
    "NN_model_3.compile( \n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizer\n",
    "  loss=keras.losses.MeanAbsoluteError(), # Loss function to minimize\n",
    "  metrics=['mape', 'mse'] # metrics\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dV14ejNAdwM",
    "outputId": "ef204caa-d436-4cd0-f446-dd46ae19696b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 512)               11776     \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 256)               131328    \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 256)               65792     \n",
      "                                                                 \n",
      " layer4 (Dense)              (None, 128)               32896     \n",
      "                                                                 \n",
      " layer5 (Dense)              (None, 64)                8256      \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,113\n",
      "Trainable params: 250,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Looking at the structer\n",
    "NN_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "FtBjTo9pAdyn"
   },
   "outputs": [],
   "source": [
    "# Defining callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kj3AbedhAd1X",
    "outputId": "e7047fd1-575e-4b96-8f52-a54838183e32",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 4469.5361 - mape: 28.7177 - mse: 63108576.0000\n",
      "Epoch 1: val_loss improved from inf to 3482.63037, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 4469.2100 - mape: 28.7141 - mse: 63101268.0000 - val_loss: 3482.6304 - val_mape: 21.0984 - val_mse: 36558912.0000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 2915.4441 - mape: 18.0482 - mse: 26969852.0000\n",
      "Epoch 2: val_loss improved from 3482.63037 to 2570.31934, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2914.0840 - mape: 18.0424 - mse: 26947672.0000 - val_loss: 2570.3193 - val_mape: 16.9014 - val_mse: 20902024.0000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 2493.8867 - mape: 15.6398 - mse: 21025974.0000\n",
      "Epoch 3: val_loss improved from 2570.31934 to 2379.83716, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2494.0930 - mape: 15.6385 - mse: 21030206.0000 - val_loss: 2379.8372 - val_mape: 15.1243 - val_mse: 19898260.0000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 2334.0654 - mape: 14.5955 - mse: 19186646.0000\n",
      "Epoch 4: val_loss improved from 2379.83716 to 2204.93188, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2334.3931 - mape: 14.5943 - mse: 19195580.0000 - val_loss: 2204.9319 - val_mape: 13.6838 - val_mse: 17791318.0000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 2224.2717 - mape: 13.9176 - mse: 18050806.0000\n",
      "Epoch 5: val_loss improved from 2204.93188 to 2114.77368, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2223.4185 - mape: 13.9159 - mse: 18037898.0000 - val_loss: 2114.7737 - val_mape: 13.6120 - val_mse: 16868056.0000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 2145.4080 - mape: 13.3923 - mse: 17172940.0000\n",
      "Epoch 6: val_loss improved from 2114.77368 to 2113.45703, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2144.8337 - mape: 13.3923 - mse: 17163558.0000 - val_loss: 2113.4570 - val_mape: 13.4084 - val_mse: 16838392.0000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 2069.6621 - mape: 12.9534 - mse: 16285976.0000\n",
      "Epoch 7: val_loss improved from 2113.45703 to 2015.54712, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2069.8069 - mape: 12.9551 - mse: 16288207.0000 - val_loss: 2015.5471 - val_mape: 14.0230 - val_mse: 15097120.0000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 2006.5454 - mape: 12.5651 - mse: 15654381.0000\n",
      "Epoch 8: val_loss did not improve from 2015.54712\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 2006.5485 - mape: 12.5652 - mse: 15654316.0000 - val_loss: 2100.4373 - val_mape: 14.3535 - val_mse: 15112423.0000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1953.5271 - mape: 12.2695 - mse: 15024415.0000\n",
      "Epoch 9: val_loss did not improve from 2015.54712\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1953.7510 - mape: 12.2683 - mse: 15030910.0000 - val_loss: 2168.6433 - val_mape: 12.4824 - val_mse: 17751094.0000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1904.5367 - mape: 11.9982 - mse: 14535758.0000\n",
      "Epoch 10: val_loss improved from 2015.54712 to 1863.48779, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1904.5367 - mape: 11.9982 - mse: 14535758.0000 - val_loss: 1863.4878 - val_mape: 11.8129 - val_mse: 14580434.0000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1851.4813 - mape: 11.6909 - mse: 13963229.0000\n",
      "Epoch 11: val_loss improved from 1863.48779 to 1810.10547, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1851.5720 - mape: 11.6910 - mse: 13965964.0000 - val_loss: 1810.1055 - val_mape: 11.8881 - val_mse: 13248724.0000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1813.8628 - mape: 11.5020 - mse: 13455741.0000\n",
      "Epoch 12: val_loss did not improve from 1810.10547\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1813.8928 - mape: 11.5023 - mse: 13455107.0000 - val_loss: 1951.7639 - val_mape: 12.2417 - val_mse: 13661247.0000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1769.4916 - mape: 11.2402 - mse: 12922625.0000\n",
      "Epoch 13: val_loss improved from 1810.10547 to 1751.49927, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1769.4803 - mape: 11.2387 - mse: 12922093.0000 - val_loss: 1751.4993 - val_mape: 10.9618 - val_mse: 12908278.0000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1724.3999 - mape: 11.0247 - mse: 12365269.0000\n",
      "Epoch 14: val_loss improved from 1751.49927 to 1657.60437, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1724.4218 - mape: 11.0241 - mse: 12364440.0000 - val_loss: 1657.6044 - val_mape: 10.7716 - val_mse: 12203650.0000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1676.9065 - mape: 10.7881 - mse: 11830167.0000\n",
      "Epoch 15: val_loss improved from 1657.60437 to 1634.54749, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1676.9132 - mape: 10.7873 - mse: 11831122.0000 - val_loss: 1634.5475 - val_mape: 10.6038 - val_mse: 11676317.0000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1641.1422 - mape: 10.5524 - mse: 11390084.0000\n",
      "Epoch 16: val_loss improved from 1634.54749 to 1622.54077, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1640.9861 - mape: 10.5544 - mse: 11386372.0000 - val_loss: 1622.5408 - val_mape: 10.8909 - val_mse: 11037610.0000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 1609.8380 - mape: 10.3604 - mse: 11077172.0000\n",
      "Epoch 17: val_loss improved from 1622.54077 to 1590.28845, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1609.6455 - mape: 10.3590 - mse: 11075719.0000 - val_loss: 1590.2885 - val_mape: 10.2319 - val_mse: 11075293.0000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1571.3711 - mape: 10.1816 - mse: 10726936.0000\n",
      "Epoch 18: val_loss did not improve from 1590.28845\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1571.4940 - mape: 10.1816 - mse: 10727052.0000 - val_loss: 1693.6667 - val_mape: 10.4126 - val_mse: 12150502.0000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1547.7072 - mape: 9.9943 - mse: 10446139.0000\n",
      "Epoch 19: val_loss improved from 1590.28845 to 1547.20972, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1547.4324 - mape: 9.9976 - mse: 10443249.0000 - val_loss: 1547.2097 - val_mape: 10.6936 - val_mse: 10160452.0000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1513.1816 - mape: 9.8320 - mse: 10100019.0000\n",
      "Epoch 20: val_loss did not improve from 1547.20972\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1513.2244 - mape: 9.8350 - mse: 10101854.0000 - val_loss: 1636.1421 - val_mape: 11.2094 - val_mse: 10105870.0000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 1490.6676 - mape: 9.6942 - mse: 10001984.0000\n",
      "Epoch 21: val_loss improved from 1547.20972 to 1473.43835, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1490.9836 - mape: 9.6944 - mse: 10005716.0000 - val_loss: 1473.4384 - val_mape: 9.4856 - val_mse: 10150311.0000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1469.8198 - mape: 9.5485 - mse: 9761166.0000\n",
      "Epoch 22: val_loss did not improve from 1473.43835\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1469.4429 - mape: 9.5497 - mse: 9755624.0000 - val_loss: 1478.3396 - val_mape: 9.6836 - val_mse: 9964058.0000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1448.4434 - mape: 9.4036 - mse: 9615307.0000\n",
      "Epoch 23: val_loss did not improve from 1473.43835\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1448.7369 - mape: 9.4064 - mse: 9620898.0000 - val_loss: 1494.8512 - val_mape: 10.0520 - val_mse: 9850485.0000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1431.8966 - mape: 9.3274 - mse: 9489616.0000\n",
      "Epoch 24: val_loss did not improve from 1473.43835\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1431.7031 - mape: 9.3274 - mse: 9485964.0000 - val_loss: 1489.2291 - val_mape: 9.7365 - val_mse: 9576967.0000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1418.2617 - mape: 9.2347 - mse: 9383711.0000\n",
      "Epoch 25: val_loss improved from 1473.43835 to 1469.73718, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1418.1210 - mape: 9.2350 - mse: 9381006.0000 - val_loss: 1469.7372 - val_mape: 9.8310 - val_mse: 9743445.0000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1397.4204 - mape: 9.1318 - mse: 9200076.0000\n",
      "Epoch 26: val_loss improved from 1469.73718 to 1397.30103, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1397.6216 - mape: 9.1326 - mse: 9201798.0000 - val_loss: 1397.3010 - val_mape: 9.3138 - val_mse: 9525145.0000 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1376.9790 - mape: 9.0191 - mse: 9026467.0000\n",
      "Epoch 27: val_loss improved from 1397.30103 to 1384.44788, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1377.2683 - mape: 9.0205 - mse: 9027820.0000 - val_loss: 1384.4479 - val_mape: 9.2533 - val_mse: 9250699.0000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 1364.0472 - mape: 8.9214 - mse: 8985147.0000\n",
      "Epoch 28: val_loss did not improve from 1384.44788\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1364.2931 - mape: 8.9231 - mse: 8983972.0000 - val_loss: 1385.0244 - val_mape: 9.1958 - val_mse: 9450259.0000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1351.4717 - mape: 8.8700 - mse: 8823442.0000\n",
      "Epoch 29: val_loss did not improve from 1384.44788\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1351.9016 - mape: 8.8732 - mse: 8826943.0000 - val_loss: 1427.6373 - val_mape: 9.2706 - val_mse: 9577259.0000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1335.3524 - mape: 8.7525 - mse: 8749157.0000\n",
      "Epoch 30: val_loss improved from 1384.44788 to 1348.65906, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1335.2014 - mape: 8.7527 - mse: 8747146.0000 - val_loss: 1348.6591 - val_mape: 9.0543 - val_mse: 9289708.0000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1320.7922 - mape: 8.6733 - mse: 8664240.0000\n",
      "Epoch 31: val_loss improved from 1348.65906 to 1338.73999, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1320.8138 - mape: 8.6733 - mse: 8664286.0000 - val_loss: 1338.7400 - val_mape: 8.9368 - val_mse: 8734807.0000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1299.8090 - mape: 8.5782 - mse: 8495806.0000\n",
      "Epoch 32: val_loss improved from 1338.73999 to 1324.55505, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1299.7942 - mape: 8.5773 - mse: 8503882.0000 - val_loss: 1324.5551 - val_mape: 9.0907 - val_mse: 8902878.0000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1292.8970 - mape: 8.5402 - mse: 8453571.0000\n",
      "Epoch 33: val_loss improved from 1324.55505 to 1307.24695, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1292.8812 - mape: 8.5402 - mse: 8453377.0000 - val_loss: 1307.2469 - val_mape: 9.0545 - val_mse: 8698429.0000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 1282.2013 - mape: 8.4735 - mse: 8357494.0000\n",
      "Epoch 34: val_loss did not improve from 1307.24695\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1282.1782 - mape: 8.4734 - mse: 8357283.5000 - val_loss: 1367.5840 - val_mape: 9.3163 - val_mse: 8612622.0000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 1271.9390 - mape: 8.3917 - mse: 8301017.0000\n",
      "Epoch 35: val_loss did not improve from 1307.24695\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1271.9650 - mape: 8.3922 - mse: 8301373.5000 - val_loss: 1310.4304 - val_mape: 8.8845 - val_mse: 9024317.0000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1253.2814 - mape: 8.3093 - mse: 8195045.5000\n",
      "Epoch 36: val_loss did not improve from 1307.24695\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1253.9637 - mape: 8.3103 - mse: 8204145.5000 - val_loss: 1325.4167 - val_mape: 8.7470 - val_mse: 9212128.0000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1246.1919 - mape: 8.2552 - mse: 8154783.0000\n",
      "Epoch 37: val_loss did not improve from 1307.24695\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1246.4563 - mape: 8.2560 - mse: 8157346.5000 - val_loss: 1312.9050 - val_mape: 8.8090 - val_mse: 9149129.0000 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 1237.7418 - mape: 8.2041 - mse: 8098408.0000\n",
      "Epoch 38: val_loss improved from 1307.24695 to 1299.37158, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1237.7671 - mape: 8.2051 - mse: 8097931.0000 - val_loss: 1299.3716 - val_mape: 8.8136 - val_mse: 8836583.0000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1226.1693 - mape: 8.1238 - mse: 7990478.5000\n",
      "Epoch 39: val_loss improved from 1299.37158 to 1260.46106, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1226.6860 - mape: 8.1262 - mse: 7999241.5000 - val_loss: 1260.4611 - val_mape: 8.6953 - val_mse: 8582571.0000 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1214.5006 - mape: 8.0563 - mse: 7944625.0000\n",
      "Epoch 40: val_loss did not improve from 1260.46106\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1214.5314 - mape: 8.0570 - mse: 7944744.0000 - val_loss: 1290.8395 - val_mape: 8.5606 - val_mse: 9050886.0000 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 1204.2957 - mape: 7.9962 - mse: 7860888.5000\n",
      "Epoch 41: val_loss did not improve from 1260.46106\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1204.2766 - mape: 7.9965 - mse: 7859682.0000 - val_loss: 1262.0366 - val_mape: 8.5858 - val_mse: 8849060.0000 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1199.2378 - mape: 7.9655 - mse: 7782833.0000\n",
      "Epoch 42: val_loss did not improve from 1260.46106\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1199.4580 - mape: 7.9648 - mse: 7791566.0000 - val_loss: 1333.9657 - val_mape: 8.6303 - val_mse: 9245095.0000 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 1187.4030 - mape: 7.9021 - mse: 7790704.0000\n",
      "Epoch 43: val_loss did not improve from 1260.46106\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 1187.6238 - mape: 7.9013 - mse: 7793379.0000 - val_loss: 1389.5468 - val_mape: 8.6593 - val_mse: 9806794.0000 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 1182.0720 - mape: 7.8665 - mse: 7705594.5000\n",
      "Epoch 44: val_loss did not improve from 1260.46106\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1182.0859 - mape: 7.8686 - mse: 7707147.0000 - val_loss: 1442.9879 - val_mape: 9.1372 - val_mse: 8547061.0000 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 1172.6621 - mape: 7.8230 - mse: 7670036.5000\n",
      "Epoch 45: val_loss improved from 1260.46106 to 1222.22888, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1173.0974 - mape: 7.8238 - mse: 7681764.0000 - val_loss: 1222.2289 - val_mape: 8.3807 - val_mse: 8284123.0000 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 1166.5220 - mape: 7.7785 - mse: 7635763.5000\n",
      "Epoch 46: val_loss did not improve from 1222.22888\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1166.3185 - mape: 7.7773 - mse: 7634280.0000 - val_loss: 1330.6771 - val_mape: 8.5468 - val_mse: 8300734.0000 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1160.1038 - mape: 7.7080 - mse: 7564871.0000\n",
      "Epoch 47: val_loss improved from 1222.22888 to 1201.18347, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1160.0042 - mape: 7.7094 - mse: 7560994.5000 - val_loss: 1201.1835 - val_mape: 8.1999 - val_mse: 8350844.5000 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 1155.8279 - mape: 7.7011 - mse: 7520148.0000\n",
      "Epoch 48: val_loss improved from 1201.18347 to 1193.07910, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1155.7114 - mape: 7.7005 - mse: 7518351.5000 - val_loss: 1193.0791 - val_mape: 8.2596 - val_mse: 8254043.0000 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 1145.3730 - mape: 7.6357 - mse: 7456204.5000\n",
      "Epoch 49: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1145.1705 - mape: 7.6336 - mse: 7455040.5000 - val_loss: 1230.5912 - val_mape: 8.2839 - val_mse: 8461686.0000 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 1133.8583 - mape: 7.6004 - mse: 7416857.0000\n",
      "Epoch 50: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1133.8827 - mape: 7.6004 - mse: 7416610.5000 - val_loss: 1214.7800 - val_mape: 8.3028 - val_mse: 8445124.0000 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 1131.8444 - mape: 7.5516 - mse: 7377555.5000\n",
      "Epoch 51: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1131.7687 - mape: 7.5510 - mse: 7376446.0000 - val_loss: 1277.0830 - val_mape: 8.3995 - val_mse: 8082933.0000 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 1128.3340 - mape: 7.5381 - mse: 7357200.0000\n",
      "Epoch 52: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1128.3340 - mape: 7.5381 - mse: 7357200.0000 - val_loss: 1203.0094 - val_mape: 8.3088 - val_mse: 8051255.5000 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1119.0555 - mape: 7.4906 - mse: 7265577.5000\n",
      "Epoch 53: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1118.9158 - mape: 7.4891 - mse: 7262480.0000 - val_loss: 1229.6256 - val_mape: 8.1514 - val_mse: 8860746.0000 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 1114.5454 - mape: 7.4349 - mse: 7244549.5000\n",
      "Epoch 54: val_loss did not improve from 1193.07910\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1114.6962 - mape: 7.4364 - mse: 7245416.0000 - val_loss: 1235.8457 - val_mape: 8.2718 - val_mse: 8768153.0000 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 1109.8868 - mape: 7.4376 - mse: 7251272.5000\n",
      "Epoch 55: val_loss did not improve from 1193.07910\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 1109.7368 - mape: 7.4373 - mse: 7249478.5000 - val_loss: 1290.2039 - val_mape: 8.4130 - val_mse: 8418925.0000 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 926.5934 - mape: 6.3227 - mse: 6570548.0000\n",
      "Epoch 56: val_loss improved from 1193.07910 to 1025.48804, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 926.2841 - mape: 6.3203 - mse: 6567625.0000 - val_loss: 1025.4880 - val_mape: 7.1227 - val_mse: 7796171.0000 - lr: 1.0000e-04\n",
      "Epoch 57/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 882.7775 - mape: 6.0269 - mse: 6433394.5000\n",
      "Epoch 57: val_loss improved from 1025.48804 to 1013.04285, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 882.7294 - mape: 6.0260 - mse: 6435234.0000 - val_loss: 1013.0428 - val_mape: 7.0325 - val_mse: 7773270.0000 - lr: 1.0000e-04\n",
      "Epoch 58/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 869.8049 - mape: 5.9454 - mse: 6394635.5000\n",
      "Epoch 58: val_loss improved from 1013.04285 to 1011.68555, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 870.2401 - mape: 5.9443 - mse: 6405848.5000 - val_loss: 1011.6855 - val_mape: 7.0204 - val_mse: 7775213.5000 - lr: 1.0000e-04\n",
      "Epoch 59/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 862.8324 - mape: 5.8940 - mse: 6386220.0000\n",
      "Epoch 59: val_loss did not improve from 1011.68555\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 862.8539 - mape: 5.8940 - mse: 6386365.0000 - val_loss: 1026.2939 - val_mape: 7.0067 - val_mse: 7933889.0000 - lr: 1.0000e-04\n",
      "Epoch 60/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 857.3765 - mape: 5.8518 - mse: 6376901.5000\n",
      "Epoch 60: val_loss improved from 1011.68555 to 1004.66858, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 857.2616 - mape: 5.8517 - mse: 6373919.0000 - val_loss: 1004.6686 - val_mape: 7.0002 - val_mse: 7734653.5000 - lr: 1.0000e-04\n",
      "Epoch 61/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 852.6047 - mape: 5.8197 - mse: 6364068.0000\n",
      "Epoch 61: val_loss did not improve from 1004.66858\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 852.5884 - mape: 5.8196 - mse: 6363904.5000 - val_loss: 1006.2770 - val_mape: 6.9841 - val_mse: 7691520.5000 - lr: 1.0000e-04\n",
      "Epoch 62/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 847.9948 - mape: 5.7894 - mse: 6342315.0000\n",
      "Epoch 62: val_loss improved from 1004.66858 to 999.20105, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 848.3394 - mape: 5.7912 - mse: 6346006.0000 - val_loss: 999.2010 - val_mape: 6.9501 - val_mse: 7761123.0000 - lr: 1.0000e-04\n",
      "Epoch 63/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 845.2169 - mape: 5.7716 - mse: 6337356.0000\n",
      "Epoch 63: val_loss did not improve from 999.20105\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 845.2712 - mape: 5.7717 - mse: 6340286.0000 - val_loss: 1004.7883 - val_mape: 6.9407 - val_mse: 7862507.5000 - lr: 1.0000e-04\n",
      "Epoch 64/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 842.6510 - mape: 5.7502 - mse: 6336004.0000\n",
      "Epoch 64: val_loss improved from 999.20105 to 993.41473, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 842.2861 - mape: 5.7500 - mse: 6330637.0000 - val_loss: 993.4147 - val_mape: 6.9194 - val_mse: 7775002.5000 - lr: 1.0000e-04\n",
      "Epoch 65/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 839.1032 - mape: 5.7273 - mse: 6310923.5000\n",
      "Epoch 65: val_loss did not improve from 993.41473\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 839.1032 - mape: 5.7273 - mse: 6310923.5000 - val_loss: 993.4154 - val_mape: 6.9074 - val_mse: 7810753.0000 - lr: 1.0000e-04\n",
      "Epoch 66/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 836.4269 - mape: 5.7058 - mse: 6313163.5000\n",
      "Epoch 66: val_loss improved from 993.41473 to 993.31891, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 836.5529 - mape: 5.7075 - mse: 6310499.5000 - val_loss: 993.3189 - val_mape: 6.9017 - val_mse: 7815780.5000 - lr: 1.0000e-04\n",
      "Epoch 67/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 834.7228 - mape: 5.6906 - mse: 6301961.0000\n",
      "Epoch 67: val_loss improved from 993.31891 to 989.75208, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 834.6495 - mape: 5.6900 - mse: 6299879.0000 - val_loss: 989.7521 - val_mape: 6.9067 - val_mse: 7826657.0000 - lr: 1.0000e-04\n",
      "Epoch 68/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 831.3472 - mape: 5.6690 - mse: 6296399.0000\n",
      "Epoch 68: val_loss improved from 989.75208 to 989.33057, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 831.6716 - mape: 5.6699 - mse: 6297899.0000 - val_loss: 989.3306 - val_mape: 6.9153 - val_mse: 7794808.0000 - lr: 1.0000e-04\n",
      "Epoch 69/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 829.8054 - mape: 5.6576 - mse: 6290014.5000\n",
      "Epoch 69: val_loss did not improve from 989.33057\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 829.8080 - mape: 5.6575 - mse: 6289952.0000 - val_loss: 994.8339 - val_mape: 6.9414 - val_mse: 7734184.5000 - lr: 1.0000e-04\n",
      "Epoch 70/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 827.6148 - mape: 5.6441 - mse: 6277118.5000\n",
      "Epoch 70: val_loss did not improve from 989.33057\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 827.5125 - mape: 5.6434 - mse: 6278871.0000 - val_loss: 989.9213 - val_mape: 6.8303 - val_mse: 7795738.5000 - lr: 1.0000e-04\n",
      "Epoch 71/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 826.2025 - mape: 5.6317 - mse: 6271311.0000\n",
      "Epoch 71: val_loss improved from 989.33057 to 988.28589, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 825.9262 - mape: 5.6308 - mse: 6268225.5000 - val_loss: 988.2859 - val_mape: 6.8595 - val_mse: 7766790.0000 - lr: 1.0000e-04\n",
      "Epoch 72/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 823.8193 - mape: 5.6148 - mse: 6269915.0000\n",
      "Epoch 72: val_loss improved from 988.28589 to 985.33667, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 823.7795 - mape: 5.6140 - mse: 6268243.5000 - val_loss: 985.3367 - val_mape: 6.8521 - val_mse: 7773780.5000 - lr: 1.0000e-04\n",
      "Epoch 73/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 822.1281 - mape: 5.6027 - mse: 6251483.5000\n",
      "Epoch 73: val_loss did not improve from 985.33667\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 822.1779 - mape: 5.6027 - mse: 6251150.5000 - val_loss: 987.2534 - val_mape: 6.8543 - val_mse: 7829982.5000 - lr: 1.0000e-04\n",
      "Epoch 74/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 820.9923 - mape: 5.5927 - mse: 6254794.0000\n",
      "Epoch 74: val_loss did not improve from 985.33667\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 820.7751 - mape: 5.5921 - mse: 6251496.5000 - val_loss: 986.0928 - val_mape: 6.8343 - val_mse: 7788165.0000 - lr: 1.0000e-04\n",
      "Epoch 75/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 818.6395 - mape: 5.5800 - mse: 6234769.0000\n",
      "Epoch 75: val_loss improved from 985.33667 to 984.73767, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 818.5283 - mape: 5.5790 - mse: 6233521.0000 - val_loss: 984.7377 - val_mape: 6.8294 - val_mse: 7835521.0000 - lr: 1.0000e-04\n",
      "Epoch 76/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 817.3669 - mape: 5.5693 - mse: 6234353.5000\n",
      "Epoch 76: val_loss did not improve from 984.73767\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 817.2664 - mape: 5.5690 - mse: 6233201.5000 - val_loss: 990.5344 - val_mape: 6.8704 - val_mse: 7872347.5000 - lr: 1.0000e-04\n",
      "Epoch 77/500\n",
      "6003/6006 [============================>.] - ETA: 0s - loss: 815.5806 - mape: 5.5577 - mse: 6221119.0000\n",
      "Epoch 77: val_loss improved from 984.73767 to 982.65485, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 815.6063 - mape: 5.5581 - mse: 6222396.0000 - val_loss: 982.6548 - val_mape: 6.8878 - val_mse: 7798928.5000 - lr: 1.0000e-04\n",
      "Epoch 78/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 814.2272 - mape: 5.5487 - mse: 6214980.5000\n",
      "Epoch 78: val_loss did not improve from 982.65485\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 814.0908 - mape: 5.5485 - mse: 6213175.5000 - val_loss: 993.8967 - val_mape: 6.8566 - val_mse: 7889407.0000 - lr: 1.0000e-04\n",
      "Epoch 79/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 812.7517 - mape: 5.5364 - mse: 6207641.5000\n",
      "Epoch 79: val_loss did not improve from 982.65485\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 812.6033 - mape: 5.5365 - mse: 6204216.0000 - val_loss: 990.5956 - val_mape: 6.8967 - val_mse: 7731568.5000 - lr: 1.0000e-04\n",
      "Epoch 80/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 811.6489 - mape: 5.5244 - mse: 6202878.5000\n",
      "Epoch 80: val_loss did not improve from 982.65485\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 811.5312 - mape: 5.5255 - mse: 6199585.0000 - val_loss: 985.8059 - val_mape: 6.8261 - val_mse: 7757534.5000 - lr: 1.0000e-04\n",
      "Epoch 81/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 810.2325 - mape: 5.5174 - mse: 6193189.0000\n",
      "Epoch 81: val_loss did not improve from 982.65485\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 810.4532 - mape: 5.5164 - mse: 6198357.5000 - val_loss: 985.3946 - val_mape: 6.8218 - val_mse: 7765614.5000 - lr: 1.0000e-04\n",
      "Epoch 82/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 808.2270 - mape: 5.5016 - mse: 6173219.0000\n",
      "Epoch 82: val_loss improved from 982.65485 to 979.85455, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 808.6208 - mape: 5.5031 - mse: 6181629.5000 - val_loss: 979.8546 - val_mape: 6.7979 - val_mse: 7797642.0000 - lr: 1.0000e-04\n",
      "Epoch 83/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 806.7570 - mape: 5.4927 - mse: 6174658.0000\n",
      "Epoch 83: val_loss improved from 979.85455 to 978.34137, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 806.7059 - mape: 5.4934 - mse: 6172799.0000 - val_loss: 978.3414 - val_mape: 6.8060 - val_mse: 7829866.5000 - lr: 1.0000e-04\n",
      "Epoch 84/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 806.0178 - mape: 5.4878 - mse: 6168841.5000\n",
      "Epoch 84: val_loss did not improve from 978.34137\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 805.9205 - mape: 5.4882 - mse: 6166501.5000 - val_loss: 982.7780 - val_mape: 6.8273 - val_mse: 7778431.0000 - lr: 1.0000e-04\n",
      "Epoch 85/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 804.9866 - mape: 5.4797 - mse: 6161027.5000\n",
      "Epoch 85: val_loss did not improve from 978.34137\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 805.1840 - mape: 5.4800 - mse: 6170548.5000 - val_loss: 982.9013 - val_mape: 6.8145 - val_mse: 7781642.0000 - lr: 1.0000e-04\n",
      "Epoch 86/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 803.9122 - mape: 5.4698 - mse: 6164741.0000\n",
      "Epoch 86: val_loss improved from 978.34137 to 977.57928, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 803.8566 - mape: 5.4700 - mse: 6161876.5000 - val_loss: 977.5793 - val_mape: 6.8217 - val_mse: 7803296.0000 - lr: 1.0000e-04\n",
      "Epoch 87/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 802.8954 - mape: 5.4618 - mse: 6155000.5000\n",
      "Epoch 87: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 802.5414 - mape: 5.4605 - mse: 6150563.5000 - val_loss: 980.1680 - val_mape: 6.7658 - val_mse: 7823668.5000 - lr: 1.0000e-04\n",
      "Epoch 88/500\n",
      "5996/6006 [============================>.] - ETA: 0s - loss: 801.2476 - mape: 5.4525 - mse: 6143556.5000\n",
      "Epoch 88: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 801.2818 - mape: 5.4536 - mse: 6142418.0000 - val_loss: 978.2734 - val_mape: 6.8113 - val_mse: 7859594.0000 - lr: 1.0000e-04\n",
      "Epoch 89/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 800.4175 - mape: 5.4413 - mse: 6146575.0000\n",
      "Epoch 89: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 800.1033 - mape: 5.4410 - mse: 6144452.0000 - val_loss: 978.0958 - val_mape: 6.8058 - val_mse: 7814844.0000 - lr: 1.0000e-04\n",
      "Epoch 90/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 798.9343 - mape: 5.4378 - mse: 6126837.0000\n",
      "Epoch 90: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 799.1314 - mape: 5.4381 - mse: 6130903.0000 - val_loss: 979.2452 - val_mape: 6.7765 - val_mse: 7807029.0000 - lr: 1.0000e-04\n",
      "Epoch 91/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 798.4411 - mape: 5.4303 - mse: 6130967.5000\n",
      "Epoch 91: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 798.2658 - mape: 5.4294 - mse: 6126642.0000 - val_loss: 977.7108 - val_mape: 6.8004 - val_mse: 7784961.0000 - lr: 1.0000e-04\n",
      "Epoch 92/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 797.0990 - mape: 5.4199 - mse: 6112082.5000\n",
      "Epoch 92: val_loss did not improve from 977.57928\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 797.3901 - mape: 5.4211 - mse: 6113794.0000 - val_loss: 980.6774 - val_mape: 6.8375 - val_mse: 7765210.5000 - lr: 1.0000e-04\n",
      "Epoch 93/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 795.6752 - mape: 5.4113 - mse: 6114494.5000\n",
      "Epoch 93: val_loss did not improve from 977.57928\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 795.6752 - mape: 5.4113 - mse: 6114494.5000 - val_loss: 982.8711 - val_mape: 6.7607 - val_mse: 7866273.5000 - lr: 1.0000e-04\n",
      "Epoch 94/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 775.7375 - mape: 5.2895 - mse: 6064277.0000\n",
      "Epoch 94: val_loss improved from 977.57928 to 966.18378, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 775.6395 - mape: 5.2893 - mse: 6060722.0000 - val_loss: 966.1838 - val_mape: 6.7102 - val_mse: 7795734.5000 - lr: 1.0000e-05\n",
      "Epoch 95/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 771.3590 - mape: 5.2621 - mse: 6057935.5000\n",
      "Epoch 95: val_loss improved from 966.18378 to 964.42755, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 771.2955 - mape: 5.2617 - mse: 6055955.5000 - val_loss: 964.4276 - val_mape: 6.6990 - val_mse: 7810793.0000 - lr: 1.0000e-05\n",
      "Epoch 96/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 769.9247 - mape: 5.2530 - mse: 6051315.0000\n",
      "Epoch 96: val_loss did not improve from 964.42755\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 769.9825 - mape: 5.2535 - mse: 6053979.5000 - val_loss: 965.8359 - val_mape: 6.7002 - val_mse: 7838516.0000 - lr: 1.0000e-05\n",
      "Epoch 97/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 769.3244 - mape: 5.2492 - mse: 6058008.5000\n",
      "Epoch 97: val_loss improved from 964.42755 to 964.42139, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 769.3145 - mape: 5.2492 - mse: 6057860.5000 - val_loss: 964.4214 - val_mape: 6.7045 - val_mse: 7817965.0000 - lr: 1.0000e-05\n",
      "Epoch 98/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 768.5361 - mape: 5.2459 - mse: 6049435.0000\n",
      "Epoch 98: val_loss improved from 964.42139 to 963.52991, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 768.7457 - mape: 5.2453 - mse: 6056085.0000 - val_loss: 963.5299 - val_mape: 6.6972 - val_mse: 7814541.0000 - lr: 1.0000e-05\n",
      "Epoch 99/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 768.3749 - mape: 5.2436 - mse: 6054084.5000\n",
      "Epoch 99: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 768.3530 - mape: 5.2431 - mse: 6055920.0000 - val_loss: 964.0869 - val_mape: 6.6955 - val_mse: 7825243.5000 - lr: 1.0000e-05\n",
      "Epoch 100/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 767.9846 - mape: 5.2413 - mse: 6056132.5000\n",
      "Epoch 100: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 767.9601 - mape: 5.2410 - mse: 6055365.5000 - val_loss: 963.6909 - val_mape: 6.7045 - val_mse: 7812046.0000 - lr: 1.0000e-05\n",
      "Epoch 101/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 767.8735 - mape: 5.2389 - mse: 6056214.0000\n",
      "Epoch 101: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 767.6823 - mape: 5.2392 - mse: 6053726.5000 - val_loss: 963.5597 - val_mape: 6.6961 - val_mse: 7815553.5000 - lr: 1.0000e-05\n",
      "Epoch 102/500\n",
      "5999/6006 [============================>.] - ETA: 0s - loss: 767.5165 - mape: 5.2371 - mse: 6053297.5000\n",
      "Epoch 102: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 767.3337 - mape: 5.2367 - mse: 6053586.0000 - val_loss: 963.8799 - val_mape: 6.7085 - val_mse: 7813559.0000 - lr: 1.0000e-05\n",
      "Epoch 103/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 766.7231 - mape: 5.2357 - mse: 6047585.5000\n",
      "Epoch 103: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 767.1077 - mape: 5.2358 - mse: 6051413.5000 - val_loss: 963.5516 - val_mape: 6.7016 - val_mse: 7805793.5000 - lr: 1.0000e-05\n",
      "Epoch 104/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 766.4539 - mape: 5.2330 - mse: 6048234.5000\n",
      "Epoch 104: val_loss did not improve from 963.52991\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 766.8846 - mape: 5.2336 - mse: 6052050.5000 - val_loss: 964.3794 - val_mape: 6.7098 - val_mse: 7832018.0000 - lr: 1.0000e-05\n",
      "Epoch 105/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 766.6584 - mape: 5.2325 - mse: 6053362.5000\n",
      "Epoch 105: val_loss did not improve from 963.52991\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 766.6190 - mape: 5.2325 - mse: 6052638.0000 - val_loss: 963.7791 - val_mape: 6.7001 - val_mse: 7824193.0000 - lr: 1.0000e-05\n",
      "Epoch 106/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 764.3226 - mape: 5.2183 - mse: 6041821.0000\n",
      "Epoch 106: val_loss improved from 963.52991 to 963.08673, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 764.2050 - mape: 5.2180 - mse: 6043391.0000 - val_loss: 963.0867 - val_mape: 6.6980 - val_mse: 7817264.5000 - lr: 1.0000e-06\n",
      "Epoch 107/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 763.6379 - mape: 5.2152 - mse: 6035926.0000\n",
      "Epoch 107: val_loss improved from 963.08673 to 962.92358, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.9089 - mape: 5.2156 - mse: 6042970.5000 - val_loss: 962.9236 - val_mape: 6.7003 - val_mse: 7815736.0000 - lr: 1.0000e-06\n",
      "Epoch 108/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 764.1544 - mape: 5.2150 - mse: 6049957.5000\n",
      "Epoch 108: val_loss did not improve from 962.92358\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.7782 - mape: 5.2154 - mse: 6043555.0000 - val_loss: 962.9322 - val_mape: 6.7003 - val_mse: 7812051.0000 - lr: 1.0000e-06\n",
      "Epoch 109/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 763.7890 - mape: 5.2146 - mse: 6045806.0000\n",
      "Epoch 109: val_loss did not improve from 962.92358\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.6897 - mape: 5.2143 - mse: 6044720.0000 - val_loss: 962.9359 - val_mape: 6.7007 - val_mse: 7813935.0000 - lr: 1.0000e-06\n",
      "Epoch 110/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 763.6915 - mape: 5.2150 - mse: 6045711.0000\n",
      "Epoch 110: val_loss improved from 962.92358 to 962.87964, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.6326 - mape: 5.2147 - mse: 6044071.0000 - val_loss: 962.8796 - val_mape: 6.6968 - val_mse: 7815158.0000 - lr: 1.0000e-06\n",
      "Epoch 111/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 763.1143 - mape: 5.2118 - mse: 6034525.0000\n",
      "Epoch 111: val_loss did not improve from 962.87964\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.5941 - mape: 5.2137 - mse: 6043809.0000 - val_loss: 962.9061 - val_mape: 6.7011 - val_mse: 7810644.0000 - lr: 1.0000e-06\n",
      "Epoch 112/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 763.5599 - mape: 5.2136 - mse: 6044120.5000\n",
      "Epoch 112: val_loss improved from 962.87964 to 962.86517, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.5599 - mape: 5.2136 - mse: 6044120.5000 - val_loss: 962.8652 - val_mape: 6.6983 - val_mse: 7815799.0000 - lr: 1.0000e-06\n",
      "Epoch 113/500\n",
      "5992/6006 [============================>.] - ETA: 0s - loss: 763.5969 - mape: 5.2151 - mse: 6045517.0000\n",
      "Epoch 113: val_loss did not improve from 962.86517\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.5190 - mape: 5.2135 - mse: 6043901.5000 - val_loss: 962.8666 - val_mape: 6.6973 - val_mse: 7816437.5000 - lr: 1.0000e-06\n",
      "Epoch 114/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 763.4908 - mape: 5.2132 - mse: 6044319.5000\n",
      "Epoch 114: val_loss did not improve from 962.86517\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.4728 - mape: 5.2131 - mse: 6044162.5000 - val_loss: 962.8924 - val_mape: 6.6977 - val_mse: 7816525.5000 - lr: 1.0000e-06\n",
      "Epoch 115/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 763.7189 - mape: 5.2152 - mse: 6042094.0000\n",
      "Epoch 115: val_loss did not improve from 962.86517\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.4396 - mape: 5.2128 - mse: 6044623.5000 - val_loss: 963.0607 - val_mape: 6.6997 - val_mse: 7806594.5000 - lr: 1.0000e-06\n",
      "Epoch 116/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 763.5481 - mape: 5.2127 - mse: 6046394.5000\n",
      "Epoch 116: val_loss did not improve from 962.86517\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.4054 - mape: 5.2127 - mse: 6044335.0000 - val_loss: 962.9263 - val_mape: 6.7007 - val_mse: 7808528.0000 - lr: 1.0000e-06\n",
      "Epoch 117/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 763.1564 - mape: 5.2118 - mse: 6037685.0000\n",
      "Epoch 117: val_loss improved from 962.86517 to 962.82477, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.3737 - mape: 5.2126 - mse: 6044232.0000 - val_loss: 962.8248 - val_mape: 6.6979 - val_mse: 7810327.5000 - lr: 1.0000e-06\n",
      "Epoch 118/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 763.4623 - mape: 5.2123 - mse: 6046377.5000\n",
      "Epoch 118: val_loss improved from 962.82477 to 962.78595, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 763.3477 - mape: 5.2120 - mse: 6044229.0000 - val_loss: 962.7859 - val_mape: 6.6991 - val_mse: 7812823.5000 - lr: 1.0000e-06\n",
      "Epoch 119/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 763.4438 - mape: 5.2124 - mse: 6046592.5000\n",
      "Epoch 119: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.3040 - mape: 5.2120 - mse: 6044452.0000 - val_loss: 962.8192 - val_mape: 6.6985 - val_mse: 7812128.5000 - lr: 1.0000e-06\n",
      "Epoch 120/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 763.4990 - mape: 5.2109 - mse: 6048262.5000\n",
      "Epoch 120: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.2952 - mape: 5.2119 - mse: 6043803.5000 - val_loss: 962.8703 - val_mape: 6.6997 - val_mse: 7814718.5000 - lr: 1.0000e-06\n",
      "Epoch 121/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 763.0715 - mape: 5.2132 - mse: 6043618.0000\n",
      "Epoch 121: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.2452 - mape: 5.2116 - mse: 6044116.0000 - val_loss: 962.8602 - val_mape: 6.6982 - val_mse: 7811044.0000 - lr: 1.0000e-06\n",
      "Epoch 122/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 763.1348 - mape: 5.2106 - mse: 6043010.0000\n",
      "Epoch 122: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.2221 - mape: 5.2112 - mse: 6043559.5000 - val_loss: 962.8050 - val_mape: 6.6990 - val_mse: 7815779.0000 - lr: 1.0000e-06\n",
      "Epoch 123/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 763.1080 - mape: 5.2115 - mse: 6043081.0000\n",
      "Epoch 123: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.2136 - mape: 5.2112 - mse: 6044456.0000 - val_loss: 962.8193 - val_mape: 6.6998 - val_mse: 7813116.5000 - lr: 1.0000e-06\n",
      "Epoch 124/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 763.1868 - mape: 5.2111 - mse: 6044720.5000\n",
      "Epoch 124: val_loss did not improve from 962.78595\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.1689 - mape: 5.2110 - mse: 6044563.5000 - val_loss: 962.8763 - val_mape: 6.7009 - val_mse: 7810719.5000 - lr: 1.0000e-06\n",
      "Epoch 125/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 763.1632 - mape: 5.2110 - mse: 6044249.0000\n",
      "Epoch 125: val_loss improved from 962.78595 to 962.74091, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.1370 - mape: 5.2110 - mse: 6043966.0000 - val_loss: 962.7409 - val_mape: 6.6985 - val_mse: 7813598.5000 - lr: 1.0000e-06\n",
      "Epoch 126/500\n",
      "6004/6006 [============================>.] - ETA: 0s - loss: 763.1183 - mape: 5.2099 - mse: 6044397.0000\n",
      "Epoch 126: val_loss did not improve from 962.74091\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.1219 - mape: 5.2107 - mse: 6043856.0000 - val_loss: 962.7474 - val_mape: 6.6985 - val_mse: 7812538.0000 - lr: 1.0000e-06\n",
      "Epoch 127/500\n",
      "5989/6006 [============================>.] - ETA: 0s - loss: 763.1230 - mape: 5.2095 - mse: 6046287.0000\n",
      "Epoch 127: val_loss improved from 962.74091 to 962.72534, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.0922 - mape: 5.2109 - mse: 6043953.5000 - val_loss: 962.7253 - val_mape: 6.6970 - val_mse: 7813143.0000 - lr: 1.0000e-06\n",
      "Epoch 128/500\n",
      "5991/6006 [============================>.] - ETA: 0s - loss: 762.9486 - mape: 5.2112 - mse: 6039777.5000\n",
      "Epoch 128: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.0699 - mape: 5.2104 - mse: 6043881.0000 - val_loss: 962.7662 - val_mape: 6.6969 - val_mse: 7813459.5000 - lr: 1.0000e-06\n",
      "Epoch 129/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 762.8607 - mape: 5.2108 - mse: 6040783.0000\n",
      "Epoch 129: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.0422 - mape: 5.2100 - mse: 6044110.5000 - val_loss: 962.7673 - val_mape: 6.6984 - val_mse: 7813906.0000 - lr: 1.0000e-06\n",
      "Epoch 130/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 763.0052 - mape: 5.2099 - mse: 6043990.0000\n",
      "Epoch 130: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 763.0042 - mape: 5.2101 - mse: 6043890.0000 - val_loss: 962.7812 - val_mape: 6.6989 - val_mse: 7811341.5000 - lr: 1.0000e-06\n",
      "Epoch 131/500\n",
      "5997/6006 [============================>.] - ETA: 0s - loss: 763.2918 - mape: 5.2098 - mse: 6048993.0000\n",
      "Epoch 131: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.9905 - mape: 5.2100 - mse: 6043465.0000 - val_loss: 962.7578 - val_mape: 6.6960 - val_mse: 7818861.5000 - lr: 1.0000e-06\n",
      "Epoch 132/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 762.5275 - mape: 5.2090 - mse: 6041141.5000\n",
      "Epoch 132: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.9717 - mape: 5.2094 - mse: 6044304.0000 - val_loss: 962.7882 - val_mape: 6.6983 - val_mse: 7816537.5000 - lr: 1.0000e-06\n",
      "Epoch 133/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 763.0627 - mape: 5.2117 - mse: 6045566.0000\n",
      "Epoch 133: val_loss did not improve from 962.72534\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 762.9482 - mape: 5.2099 - mse: 6043710.5000 - val_loss: 962.7806 - val_mape: 6.6958 - val_mse: 7816794.0000 - lr: 1.0000e-06\n",
      "Epoch 134/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 762.9260 - mape: 5.2091 - mse: 6043453.5000\n",
      "Epoch 134: val_loss improved from 962.72534 to 962.70020, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.9260 - mape: 5.2091 - mse: 6043453.5000 - val_loss: 962.7002 - val_mape: 6.6975 - val_mse: 7814528.0000 - lr: 1.0000e-06\n",
      "Epoch 135/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 762.5956 - mape: 5.2088 - mse: 6039983.0000\n",
      "Epoch 135: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.8759 - mape: 5.2090 - mse: 6043825.5000 - val_loss: 962.7241 - val_mape: 6.6975 - val_mse: 7814213.0000 - lr: 1.0000e-06\n",
      "Epoch 136/500\n",
      "5995/6006 [============================>.] - ETA: 0s - loss: 763.0612 - mape: 5.2099 - mse: 6046805.5000\n",
      "Epoch 136: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.8651 - mape: 5.2093 - mse: 6043276.0000 - val_loss: 962.8658 - val_mape: 6.6980 - val_mse: 7818745.5000 - lr: 1.0000e-06\n",
      "Epoch 137/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 762.7646 - mape: 5.2076 - mse: 6042425.5000\n",
      "Epoch 137: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.8389 - mape: 5.2089 - mse: 6043476.0000 - val_loss: 962.8177 - val_mape: 6.6989 - val_mse: 7815411.5000 - lr: 1.0000e-06\n",
      "Epoch 138/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 762.8090 - mape: 5.2085 - mse: 6043763.0000\n",
      "Epoch 138: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.8156 - mape: 5.2087 - mse: 6043673.0000 - val_loss: 962.7307 - val_mape: 6.6984 - val_mse: 7813406.0000 - lr: 1.0000e-06\n",
      "Epoch 139/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 762.8477 - mape: 5.2086 - mse: 6046588.5000\n",
      "Epoch 139: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.8041 - mape: 5.2089 - mse: 6043594.0000 - val_loss: 962.7344 - val_mape: 6.6970 - val_mse: 7815758.0000 - lr: 1.0000e-06\n",
      "Epoch 140/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 762.8343 - mape: 5.2086 - mse: 6044312.0000\n",
      "Epoch 140: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.7817 - mape: 5.2084 - mse: 6043347.0000 - val_loss: 962.7231 - val_mape: 6.6983 - val_mse: 7816190.5000 - lr: 1.0000e-06\n",
      "Epoch 141/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 762.9373 - mape: 5.2095 - mse: 6045903.0000\n",
      "Epoch 141: val_loss did not improve from 962.70020\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.7489 - mape: 5.2084 - mse: 6043477.5000 - val_loss: 962.7434 - val_mape: 6.6975 - val_mse: 7816732.0000 - lr: 1.0000e-06\n",
      "Epoch 142/500\n",
      "6005/6006 [============================>.] - ETA: 0s - loss: 762.4570 - mape: 5.2067 - mse: 6042519.5000\n",
      "Epoch 142: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4485 - mape: 5.2066 - mse: 6042379.5000 - val_loss: 962.7246 - val_mape: 6.6979 - val_mse: 7814468.0000 - lr: 1.0000e-07\n",
      "Epoch 143/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 762.6286 - mape: 5.2065 - mse: 6044807.0000\n",
      "Epoch 143: val_loss did not improve from 962.70020\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4285 - mape: 5.2063 - mse: 6042244.0000 - val_loss: 962.7148 - val_mape: 6.6979 - val_mse: 7814549.5000 - lr: 1.0000e-07\n",
      "Epoch 144/500\n",
      "6000/6006 [============================>.] - ETA: 0s - loss: 762.3497 - mape: 5.2052 - mse: 6043031.5000\n",
      "Epoch 144: val_loss improved from 962.70020 to 962.68481, saving model to best_model.h5\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4124 - mape: 5.2063 - mse: 6042188.5000 - val_loss: 962.6848 - val_mape: 6.6976 - val_mse: 7814096.5000 - lr: 1.0000e-07\n",
      "Epoch 145/500\n",
      "6002/6006 [============================>.] - ETA: 0s - loss: 762.3567 - mape: 5.2060 - mse: 6042211.0000\n",
      "Epoch 145: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4102 - mape: 5.2062 - mse: 6042405.0000 - val_loss: 962.7063 - val_mape: 6.6978 - val_mse: 7815024.0000 - lr: 1.0000e-07\n",
      "Epoch 146/500\n",
      "6001/6006 [============================>.] - ETA: 0s - loss: 762.3756 - mape: 5.2062 - mse: 6041900.5000\n",
      "Epoch 146: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4081 - mape: 5.2062 - mse: 6042359.0000 - val_loss: 962.6967 - val_mape: 6.6978 - val_mse: 7814537.0000 - lr: 1.0000e-07\n",
      "Epoch 147/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 762.2302 - mape: 5.2059 - mse: 6039869.5000\n",
      "Epoch 147: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.4014 - mape: 5.2062 - mse: 6042472.5000 - val_loss: 962.7196 - val_mape: 6.6979 - val_mse: 7813209.0000 - lr: 1.0000e-07\n",
      "Epoch 148/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 762.8389 - mape: 5.2074 - mse: 6048196.0000\n",
      "Epoch 148: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 4ms/step - loss: 762.4002 - mape: 5.2061 - mse: 6042266.0000 - val_loss: 962.7171 - val_mape: 6.6978 - val_mse: 7814124.5000 - lr: 1.0000e-07\n",
      "Epoch 149/500\n",
      "5990/6006 [============================>.] - ETA: 0s - loss: 762.6780 - mape: 5.2069 - mse: 6045199.0000\n",
      "Epoch 149: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3971 - mape: 5.2061 - mse: 6042404.0000 - val_loss: 962.7145 - val_mape: 6.6980 - val_mse: 7814014.0000 - lr: 1.0000e-07\n",
      "Epoch 150/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 762.3536 - mape: 5.2062 - mse: 6043335.5000\n",
      "Epoch 150: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3932 - mape: 5.2061 - mse: 6042444.0000 - val_loss: 962.7342 - val_mape: 6.6985 - val_mse: 7813348.0000 - lr: 1.0000e-07\n",
      "Epoch 151/500\n",
      "5994/6006 [============================>.] - ETA: 0s - loss: 762.3934 - mape: 5.2056 - mse: 6041778.5000\n",
      "Epoch 151: val_loss did not improve from 962.68481\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3888 - mape: 5.2063 - mse: 6042230.0000 - val_loss: 962.7128 - val_mape: 6.6980 - val_mse: 7814570.5000 - lr: 1.0000e-07\n",
      "Epoch 152/500\n",
      "5998/6006 [============================>.] - ETA: 0s - loss: 762.4592 - mape: 5.2068 - mse: 6045080.0000\n",
      "Epoch 152: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3528 - mape: 5.2060 - mse: 6042382.0000 - val_loss: 962.7191 - val_mape: 6.6980 - val_mse: 7814415.0000 - lr: 1.0000e-08\n",
      "Epoch 153/500\n",
      "5993/6006 [============================>.] - ETA: 0s - loss: 762.3297 - mape: 5.2061 - mse: 6042952.5000\n",
      "Epoch 153: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3541 - mape: 5.2060 - mse: 6042259.5000 - val_loss: 962.7195 - val_mape: 6.6980 - val_mse: 7814477.0000 - lr: 1.0000e-08\n",
      "Epoch 154/500\n",
      "6006/6006 [==============================] - ETA: 0s - loss: 762.3519 - mape: 5.2060 - mse: 6042266.0000\n",
      "Epoch 154: val_loss did not improve from 962.68481\n",
      "6006/6006 [==============================] - 21s 3ms/step - loss: 762.3519 - mape: 5.2060 - mse: 6042266.0000 - val_loss: 962.7189 - val_mape: 6.6980 - val_mse: 7814398.5000 - lr: 1.0000e-08\n",
      "Epoch 154: early stopping\n",
      "CPU times: user 1h 25min 52s, sys: 15min 3s, total: 1h 40min 55s\n",
      "Wall time: 53min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training the new model for 500 epochs\n",
    "history_3 = NN_model_3.fit(X_train_ss, y_train, epochs=500, validation_split=0.2, verbose=1, callbacks=[early_stop, model_checkpoint, reduce_lr_loss], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "xXI1iUwGcdAK",
    "outputId": "1f45fb08-8c98-4f38-bc2c-c858ce3f5613"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(history_3.history['loss'], label='train')\n",
    "plt.plot(history_3.history['val_loss'], label='Validation')\n",
    "plt.title('Mean Absolute Error for Each Training Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training Observations__\n",
    "- Model training terminated at 154 epchs with a training time of 53 minutes using premium GPUs.\n",
    "- No improvement recorded after a learning rate of $1 x {10^-}{^7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgtoYk9_cinQ",
    "outputId": "4c9abea2-cfed-46b3-91df-18f38456d08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7507/7507 [==============================] - 11s 1ms/step\n",
      "1877/1877 [==============================] - 3s 1ms/step\n",
      "Model Performance\n",
      "-----------------\n",
      "Train set R2 score: 0.9875\n",
      "Test R2 score: 0.9845\n",
      "Test set Adjusted R2 score: 0.9845\n",
      "Test set Root Mean Squared Error - RMSE: 2822.1222\n",
      "Test set Mean Absolute Error - MAE: 981.0114\n",
      "Test set Mean Absolute Precentage Error - MAPE: 0.0681\n",
      "CPU times: user 22.5 s, sys: 2.17 s, total: 24.6 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluating the final NN model\n",
    "NN_model_3_performance = evaluate_NN(NN_model_3, X_test_ss, y_test, X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved the scores and reduced the errors even further and achieved the second best MAPE of 6.81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6G1ry-FAd4G",
    "outputId": "bebf91a5-ce7a-4c74-cf3b-1c2c85d850a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN_model_3.pkl']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best NN model\n",
    "joblib.dump(NN_model_3, 'NN_model_3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnRiqvAiAd7P"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Conclusion of the NN model\n",
    "- Model __performed extremely well__ on the dataset and observed almost as good scores as the Random Forest.\n",
    "- Archived a MAPE below 10%, however, RF still did slightly better.\n",
    "- Model __did not overfit__ on the training data and generalized well to the unseen data.\n",
    "- We had to train for 53 minutes even using premium GPUs, CPU training may be too __computationally exhaustive__.\n",
    "- __Second best candidate__ for predicting flight fate.\n",
    "- Has the Potential to improve to achieve the best model by making the NN deeper with extra hidden layers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRrKIi2ZAeF2"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
